- Code Availability: https://github.com/abidlabs/contrastive
  Description: "A modified version of PCA, where the covariance matrix (COV) is the\
    \ difference between COV(case/perturbed) and \u03B1COV(control/background). The\
    \ hyperparameter \u03B1 is used to balance having a high case variance and a low\
    \ control variance. To provide some intuition, when \u03B1 is 0, the model reduces\
    \ to classic PCA on the case data. Optimal alphas (equal to k clusters) are identified\
    \ using spectral clustering over a range of cPCA runs with different alphas, with\
    \ selection based on the similarity of cPCA outputs."
  Inspired by:
  - PCA
  - Contrastive Mixture Models (Zou et al., 2013)
  Method: cPCA
  Model:
  - Modified PCA
  Publication: https://www.nature.com/articles/s41467-018-04608-8#Sec7
  Published: true
  Task:
  - Linear Gene Programmes
  - Contrastive Disentanglement
  Year: 2018
- Code Availability: https://www.zhanglab-amss.org/homepage/software.html
  Description: A non-negative matrix factorisation that decomposes gene expression
    matrices into common and condition-specific patterns. For each condition, the
    observed expression matrix is approximated as the sum of a common component -
    represented by a common feature matrix with condition-specific coefficient matrices
    - and a specific component unique to each condition, represented by its own feature
    matrix  and coefficients. The model uses an alternating approach to minimize the
    combined reconstruction error (squared Frobenius norm) across common and shared
    components.
  Inspired by:
  - iNMF
  - NMF
  Method: CSMF
  Model:
  - NMF
  Publication: https://academic.oup.com/nar/article/47/13/6606/5512984
  Published: true
  Task:
  - Linear Gene Programmes
  - Contrastive Disentanglement
  Year: 2019
- Code Availability: https://github.com/kseverso/contrastive-LVM
  Description: 'A family of contrastive latent variable models (cLVMs), where case
    data are modeled as the sum of background and salient latent embeddings, while
    control data are reconstructed solely from background embeddings: - cLVM with
    Gaussian likelihoods and priors - Sparse cLVM with horseshoe prior used to regularize
    the weights - Robust cLVM with a Student''s t distribution - cLVM with automatic
    relevance determination to regularise the columns of the weight matrix - contrastive
    VAE, as a non-linear extension of the framework The shared concept across these
    models is that each model learns a shared set of latent variables for the background
    and target data, while salient latent variables are learnt solely for the target
    data.'
  Inspired by:
  - Contrastive PCA
  Method: cLVM
  Model:
  - Factor Models
  - Contastive VAE
  Publication: https://arxiv.org/abs/1811.06094
  Published: true
  Task:
  - Linear Gene Programmes
  - Contrastive Disentanglement
  Year: 2019
- Code Availability: https://github.com/abidlabs/contrastive_vae
  Description: 'VAE with two sets of latent variables (two encoders): salient and
    background, each learned using amortised inference from both case and control
    observations, respectively. The latent variables are concatenated and then decoded
    simultaneously via a shared decoder. During the generative process (decoding),
    the control observations are reconstructed solely from the background latent space,
    with salient latent variables being set to 0, while the case observations are
    generated from both sets of latent variables. Optionally, the two sets of latent
    variables can be further disentagled by minimizing their total correlation, in
    practice done by training a discriminator to distinguish real from permuted latent
    samples.'
  Inspired by:
  - Contrastive PCA
  Method: cVAE
  Model:
  - Contrastive VAE
  Publication: https://arxiv.org/pdf/1902.04601
  Published: false
  Task:
  - Contrastive Disentanglement
  Year: 2019
- Code Availability: https://github.com/PhilBoileau/EHDBDscPCA
  Description: A sparse version of contrastive PCA that enhances interpretability
    in high-dimensional settings by integrating l1 regularisation into an iterative
    procedure to estimate sparse loadings and principal components
  Inspired by:
  - Contrastive PCA
  - Probabilistic PCA
  Method: scPCA
  Model:
  - Modified PCA
  Publication: https://academic.oup.com/bioinformatics/article/36/11/3422/5807607
  Published: true
  Task:
  - Linear Gene Programmes
  - Contrastive Disentanglement
  Year: 2020
- Code Availability: https://github.com/welch-lab/MichiGAN
  Description: "MichiGAN is a two-step approach that first uses a \u03B2-TCVAE - a\
    \ variant of the variational autoencoder that penalizes total correlation among\
    \ latent variables to promote disentangled representations. These latent representations\
    \ (posterior means or samples) are then used to condition a Wasserstein GAN, the\
    \ generator of which similarly to the VAE reconstructs the data from the latent\
    \ variables, while attempting to 'fool' a discriminator whether the samples were\
    \ real or generated. Counterfactual predictions are done via latent space arithmetics\
    \ as in scGEN."
  Inspired by:
  - scGEN
  - InfoGAN
  Method: MichiGAN
  Model:
  - VAE
  - conditioned GAN
  Publication: https://link.springer.com/article/10.1186/s13059-021-02373-4
  Published: true
  Task:
  - Unsupervised Disentanglement
  - Seen Perturbation Prediction
  - Combinatorial Effect Prediction
  Year: 2021
- Code Availability: https://github.com/andrewcharlesjones/pcpca
  Description: "A probabilistic model that builds on cPCA, additionally proposing\
    \ a case-control-ratio-adjusted \u03B1 as a more interpretable alternative to\
    \ the same parameter in cPCA (see comment above)."
  Inspired by:
  - '-'
  Method: PCPCA
  Model:
  - modified PCA
  Publication: https://projecteuclid.org/journals/annals-of-applied-statistics/volume-18/issue-3/Probabilistic-contrastive-dimension-reduction-for-case-control-study-data/10.1214/24-AOAS1877.short
  Published: true
  Task:
  - Linear Gene Programmes
  - Contrastive Disentanglement
  Year: 2024
- Code Availability: https://github.com/andrewcharlesjones/cplvm
  Description: 'A family of contrastive Poisson latent variable models (CPLVMs), based
    on a Gamma-Poisson hierarchical generative process: - CPLVM: The variational posterior
    is approximated using log-normal distributions, preserving non-negativity in the
    latent factors. - CGLVM: Extends CPLVM by allowing latent factors to take negative
    values, replacing Gamma priors with Gaussian priors and using a log-link function
    for the Poisson rates. Variational posteriors are modeled as multivariate Gaussians.
    The authors also propose a hypothesis testing framework, in which log-(ELBO)-Bayes
    is calculated between a Null model, omitting the salient latent space, and the
    full contrastive model. This framework is used to quantify global (across all
    genes) and joint expression changes in subsets of genes (akin to gene set enrichment
    analysis).'
  Inspired by:
  - cPCA
  - cLVMs
  - scVI (hypothesis testing)
  Method: CPLVMs
  Model:
  - NB likelihood
  - Factor Models
  Publication: https://projecteuclid.org/journals/annals-of-applied-statistics/volume-16/issue-3/Contrastive-latent-variable-modeling-with-application-to-case-control-sequencing/10.1214/21-AOAS1534.short
  Published: true
  Task:
  - Linear Gene Programmes
  - Contrastive Disentanglement
  Year: 2022
- Code Availability: https://github.com/gemoran/sparse-vae-code
  Description: Spike and Slab Lasso applied to (non-linear) decoder weights. They
    show poofs of identifiability when at least 2 "anchor features" are present.
  Inspired by:
  - oi-VAE
  - VSC
  - beta-VAE
  Method: sparseVAE
  Model:
  - VAE
  Publication: https://arxiv.org/pdf/2110.10804
  Published: true
  Task:
  - Unsupervised Disentanglement
  Year: 2022
- Code Availability: https://github.com/scverse/scvi-tools/tree/main/src/scvi/external/contrastivevi
  Description: 'The successor to mmVAE introducing improvements: counts are modeled
    using a negative binomial distribution, and the MMD loss is replaced with the
    Wasserstein distance. More specifically, the Wasserstein distance is computed
    exclusively for the salient latent variables of the control data, ensuring it
    approaches zero. The Wasserstein penalty is optional and is set to 0 (no penalty)
    by default'
  Inspired by:
  - scVI / totalVI
  - cVAE
  - Conditional VAE
  - mmVAE (theirs)
  Method: ContrastiveVI
  Model:
  - ZINB Likelihood
  - Protein-Count (totalVI) Likelihood
  - Contrastive VAE
  - Multi-modal
  Publication: https://www.nature.com/articles/s41592-023-01955-3
  Published: true
  Task:
  - Nonlinear Gene Programmes
  - Contrastive Disentanglement
  Year: 2023
- Code Availability: https://github.com/suinleelab/MM-cVAE
  Description: A Contrastive VAE framework, similar to cVAE, which additionally incorporates
    a maximum mean discrepancy (MMD) loss to enforce salient latent variables in the
    control data to approach zero, while also using it to align the background latent
    variables between case and control conditions.
  Inspired by:
  - '-'
  Method: mmVAE
  Model:
  - Contrastive VAE
  Publication: https://arxiv.org/pdf/2202.10560
  Published: true
  Task:
  - Contrastive Disentanglement
  Year: 2022
- Code Availability: https://github.com/Genentech/multiGroupVI
  Description: An extension of ContrastiveVI to multi-case (multi-group) disentaglement
    via multiple group-specific salient encoders.
  Inspired by:
  - ContrastiveVI (theirs)
  Method: MultiGroupVI
  Model:
  - ZINB Likelihood
  - VAE
  - Contrastive
  Publication: https://proceedings.mlr.press/v200/weinberger22a
  Published: true
  Task:
  - Nonlinear Gene Programmes
  - Contrastive Disentanglement
  Year: 2022
- Code Availability: https://github.com/theislab/inVAE
  Description: 'VAE model, which incorporates technical and biological covariates
    into two sets of latent variables:  - Z_I embeds biologically-relevant variables
    - Z_B embeds the unwanted variability in the data (i.e. batch effect labels) These
    are then fed into a shared encoder, along with the count data. The output of this
    shared encoder is fed to the decoder. Optionally, further disentanglement of the
    two latent variable sets is achieved by minimizing their total correlation, which
    is approximated via a minibatch-weighted estimator that quantifies the difference
    between the joint posterior and the product of individual marginal distributions.'
  Inspired by:
  - scVI
  - iVAE
  - "\u03B2-TCVAE"
  Method: inVAE
  Model:
  - VAE
  - NB Likelihood
  Publication: https://www.biorxiv.org/content/10.1101/2024.12.06.627196v1.full
  Published: false
  Task:
  - Multi-component Disentanglement
  - Nonlinear Gene Programmes
  Year: 2024
- Code Availability: '-'
  Description: 'A VAE that disentangles disease (case) from healthy (control) cells
    by learning invariant background and salient space representations. The background
    and salient representations are summed to reconstruct the count data, with an
    (optional) interaction term capturing the interplay between cell type and disease.
    As done in contrastive methods, the salient representation for control cells is
    set to 0 during the generative (data reconstruction) process. The invariance of
    the background latent variables is enforced through two GAN-style neural networks:
    one encouraging the prediction of cell types from the background space, while
    the other penalises the prediction of disease labels, ensuring that disease-specific
    information is isolated in the salient space.'
  Inspired by:
  - DANN
  - DIVA
  - CPA
  - 'scVI '
  Method: scDSA
  Model:
  - NB likelihood
  - Domain-Adversarial NNs
  - VAE
  - Addative Shift
  Publication: https://openreview.net/pdf?id=fkoqMdTlEg
  Published: true
  Task:
  - Nonlinear Gene Programmes
  - Contrastive Disentanglement
  Year: 2023
- Code Availability: https://github.com/insitro/sams-vae
  Description: 'A VAE that encodes input data into background latent variables and
    learns sparse, global (salient) embeddings representing the effects of perturbations.
    These sparse salient embeddings are modeled using a joint relaxed straight-through
    (Beta-)Bernoulli distribution (mask) and a normally distributed latent space.
    This method captures perturbation-specific effects as an additive shift to the
    background representation, analogous to additive shift methods, but it can also
    be thought as a multi-condition extention to the contrastive framework (limited
    to two latent variables (case vs. control), to a more general setup capable of
    learning global embeddings for each perturbation. As in some contrastive methods,
    for perturbation samples, the perturbation (global) embeddings are added to the
    background latent variables to reconstruct the data, while for control samples,
    the perturbation embeddings are effectively set to zero. '
  Inspired by:
  - CPA
  - SVAE/SVAE+
  Method: SAMS-VAE
  Model:
  - VAE
  - NB likelihood
  - Conditional Latent Embeddings
  - Addative Shift
  - Sparse Mechanism Shift
  Publication: https://proceedings.neurips.cc/paper_files/paper/2023/hash/0001ca33ba34ce0351e4612b744b3936-Abstract-Conference.html
  Published: true
  Task:
  - Multi-component Disentanglement
  - Causal Structure
  - Seen Perturbation Prediction
  - Combinatorial Effect Prediction
  Year: 2023
- Code Availability: https://github.com/theislab/svaeligr
  Description: A VAE  that combines the sparse mechanism shift from SVAE+ with learning
    a probabilistic pairing between cells and unobserved auxiliary variables. These
    auxilary variables correspond to the observed perturbation labels in SVAE+, but
    here they are learned in a data-driven way (rather than passed as static labels)
    which in turn enables counterfactual context-transfer scenarios.
  Inspired by:
  - SVAE+
  Method: svae-ligr
  Model:
  - VAE
  - NB likelihood
  - Sparse Mechanism Shift
  - Generative/Experience Replay
  Publication: https://openreview.net/pdf?id=8hptqO7sfG
  Published: true
  Task:
  - Seen Perturbation Prediction
  - Context Transfer
  - Multi-component Disentanglement
  Year: 2024
- Code Availability: https://github.com/Genentech/sVAE
  Description: A VAE that integrates recent advances in sparse mechanism shift modeling
    for single-cell data, inferring a causal structure where perturbation labels identify
    the latent variables affected by each perturbation. The method constructs a graph
    identifying which latent variables are influenced by specific perturbations, promoting
    disentaglement and enabling biological interpretability, such as uncovering perturbations
    affecting shared processes. A key modelling contribution is its probabilistic
    sparsity approach (relaxed straight-through Beta-Bernoulli) on the global sparse
    embeddings (graph),  improving upon its predecessor, SVAE. As such, the latent
    space can be seen as being modelled from a Spike-and-Slab prior.
  Inspired by:
  - SVAE
  Method: sVAE+
  Model:
  - VAE
  - NB likelihood
  - Sparse Mechanism Shift
  Publication: https://proceedings.mlr.press/v213/lopez23a/lopez23a.pdf
  Published: true
  Task:
  - Seen Perturbation Prediction
  - Multi-component Disentanglement
  - Causal Structure
  - Nonlinear Gene Programmes
  Year: 2023
- Code Availability: https://github.com/bm2-lab/CausCell
  Description: CausCell integrates causal representation learning with diffusion-based
    generative modeling to generate counterfactual single-cell data. It disentangles
    observed and unobserved concepts using concept-specific adversarial discriminators
    and links the resulting latent representations through a structural causal model
    encoded as a directed acyclic graph.
  Inspired by:
  - AnnealVAE
  - DDPM
  Method: CausCell
  Model:
  - Diffusion
  - Auxilary Classifiers
  Publication: https://www.biorxiv.org/content/biorxiv/early/2024/12/17/2024.12.11.628077.full.pdf
  Published: false
  Task:
  - Multi-component Disentanglement
  - Causal Structure
  - Combinatorial Effect Prediction
  - Context Transfer
  - Seen Perturbation Prediction
  Year: 2024
- Code Availability: '-'
  Description: A VAE that combines the contrastiveVI/cVAE architecture with a classifier
    that learns the pairing of perturbation labels to cells. As in ContrastiveVI,
    unperturbed cells are drawn solely from background latent space, while cells classified
    as perturbed are reconstructed from both the background and salient sapces. Additionally,
    Hilbert-Schmidt Independence Criterion (HSIC) is used to disentagle the background
    and salient latent spaces.
  Inspired by:
  - ContrastiveVI
  - scVI
  - cVAE
  Method: SC-VAE
  Model:
  - VAE
  - NB likelihood
  Publication: https://www.biorxiv.org/content/10.1101/2024.01.05.574421v1.full
  Published: true
  Task:
  - Contrastive Disentanglement
  - Perturbation Responsiveness
  Year: 2024
- Code Availability: https://github.com/Teichlab/celcomen
  Description: Celcomen disentangles intra- and inter-cellular gene regulation in
    spatial transcriptomics data by processing gene expression through two parallel
    interaction functions. One function uses a single graph convolution layer (1-hop
    GNN) to learn a gene-gene interaction matrix that captures cross-cell signaling,
    while the other uses a linear layer to model regulation within individual cells.
    Training maximises an approximate likelihood that aligns the model-predicted weight
    matrices to the average gene expression across all cells. Simcomen then leverages
    these fixed, learned matrices to simulate spatial counterfactuals (e.g., gene
    knockouts) for in-silico experiments.
  Inspired by:
  - '-'
  Method: Celcomen
  Model:
  - K-hop Convolution
  - Spatially-informed
  Publication: https://openreview.net/pdf?id=Tqdsruwyac
  Published: true
  Task:
  - Unsupervised Disentanglement
  - Feature Relationships
  Year: 2025
- Code Availability: https://github.com/insitro/contrastive_vi_plus
  Description: An extension of ContrastiveVI that incorporates an auxiliary classifier
    to estimate the effects of perturbations, where the classifier operates on the
    salient variables and is sampled from a relaxed straight-through Bernoulli distribution.
    The output from the classifier also directly informs the salient latent space,
    indicating whether a cell expressing a gRNA successfully underwent a corresponding
    genetic perturbation. Additionally, Wasserstein distance is replaced by KL divergence,
    encouraging non-perturbed cells to map to the null region of the salient space.
    For datasets with a larger number of perturbations, the method also re-introduces
    and minimizes the Maximum Mean Discrepancy between the salient and background
    latent variables. This discourages the leakage of perturbation-induced information
    into the background latent variables, ensuring a clearer separation of perturbation
    effects.
  Inspired by:
  - ContrastiveVI (theirs)
  Method: ContrastiveVI+
  Model:
  - ZINB Likelihood
  - VAE
  - Contrastive
  Publication: https://arxiv.org/abs/2411.08072
  Published: true
  Task:
  - Perturbation Responsiveness
  - Contrastive Disentanglement
  - Nonlinear Gene Programmes
  Year: 2024
- Code Availability: https://github.com/tcapraz/SOFA
  Description: A Group Factor Analysis for multi-omics data that separates latent
    variables into guided factors, linked to predefined (observed) variables, and
    unguided factors. This structure ensures that each observed variable (known biological
    and technical effects) is captured by a corresponding guided factor, disentangling
    the observed variables from the residual information, which is in turn captured
    by the unguided factors. Additionally, SOFA works with both continous and categorical
    guiding variables and it employs a hierarchical horseshoe prior on loading weights,
    applying adaptive shrinkage at the view, factor, and feature levels.
  Inspired by:
  - MOFA+
  - Supervised Factor Analysis
  Method: SOFA
  Model:
  - ''
  - Group Factor Analysis
  Publication: https://www.biorxiv.org/content/10.1101/2024.10.10.617527v3.full
  Published: false
  Task:
  - Multi-component Disentanglement
  Year: 2024
- Code Availability: https://github.com/xinhe-lab/GSFA
  Description: "GSFA is a two-layer, guided Factor Analysis model that quantifies\
    \ the effects of genetic perturbations on latent factors. The model first factorizes\
    \ the expression matrix Y into a factor matrix Z (normal prior) and gene loadings\
    \ W (normal-mixture prior). Then, it captures the effect (\u03B2) of perturbation\
    \ on factors using multivariate linear regression. Spike-and-slab prior is used\
    \ to enforce sparsity on \u03B2, which can also analogously be seen as a causal\
    \ graph. The linearity of GSFA further enables perturbation-associated, differentially-expressed\
    \ genes to be identified. GSFA uses Gibbs sampling for inference."
  Inspired by:
  - Sparse Factor Analysis
  - Supervised factor analysis
  Method: GSFA
  Model:
  - Factor Analysis
  - Probabilistic
  Publication: https://www.nature.com/articles/s41592-023-02017-4
  Published: true
  Task:
  - Seen Perturbation Prediction
  - Multi-component Disentanglement
  - Causal Structure
  - Linear Gene Programmes
  Year: 2024
- Code Availability: https://github.com/theislab/tardis
  Description: "A VAE that partitions each cell\u2019s latent representation into\
    \ covariate-specific and covariate-agnostic (invariant) variables. It enforces\
    \ disentanglement by making the covariate-specific latents more similar for positive\
    \ pairs of cells (those sharing a covariate) and more dissimilar for negative\
    \ pairs (those differing in that covariate). Simultaneously, TarDis maximizes\
    \ or minimizes the distance between these positive/negative pairs and the covariate-agnostic\
    \ latent space in a way that ensures its independence from the targeted covariates.\
    \ This is accomplished via multiple distance-based loss terms for each covariate.\
    \ TarDis supports both categorical and continuous covariates."
  Inspired by:
  - scVI
  - IRM/ERM
  Method: TarDis
  Model:
  - VAE
  - NB likelihood
  - Multi-modal
  Publication: https://link.springer.com/chapter/10.1007/978-3-031-90252-9_23
  Published: true
  Task:
  - Multi-component Disentanglement
  - Context Transfer
  Year: 2024
- Code Availability: http://github.com/theislab/drvi
  Description: A VAE that learns disentangled latent representations in an unsupervised
    manner by employing additive decoders followed by a nonlinear pooling function
    (by default, log-sum-exp pooling). The decoder splits the latent vector into K
    variables, each decoded separately, and then aggregates these outputs. This architecture
    enforces disentanglement under theoretical assumptions, such as the additivity
    of independent processes, the existence of process-specific gene markers, and
    reconstruction quality, ensuring that distinct biological processes map to different
    latent dimensions. Additionally, DRVI performs batch-correction by optionally
    incorporating covariate information. Finally, DRVI enables the of ranking latent
    dimensions based on reconstruction and providing a gene interpretability pipeline
    via latent variable perturbations.
  Inspired by:
  - Addative Decoder ICA
  - scVI
  Method: DRVI
  Model:
  - VAE
  - NB likelihood
  - Addative Decoders
  - Multi-modal
  Publication: https://www.biorxiv.org/content/biorxiv/early/2024/11/08/2024.11.06.622266.full.pdf
  Published: false
  Task:
  - Unsupervised Disentanglement
  - Nonlinear Gene Programmes
  Year: 2024
- Code Availability: https://github.com/Genentech/fcr
  Description: 'The Factorized Causal Representation (FCR) framework disentangles
    cell representations into three latent blocks: z_x, which captures context-specific
    (covariate) effects and is invariant to treatment; z_t, which encodes direct treatment
    effects and is invariant to context; and z_{tx}, which represents interactions
    between treatment and context. It additionally handles interacting covariates
    by using a variational autoencoder framework augmented with adversarial regularisation.
    This regularisation enforces the invariance of z_x across treatments and the variability
    of z_t with respect to covariates. Moreover, the conditional independence of the
    interaction term z_{tx} from both z_x and z_t, is promoted through permutation-based
    discriminators.'
  Inspired by:
  - iVAE
  Method: FCR
  Model:
  - VAE
  - Adversarial
  - Perturbation-covariate Interactions
  Publication: https://arxiv.org/pdf/2410.22472
  Published: true
  Task:
  - Multi-component Disentanglement
  - Seen Perturbation Prediction
  Year: 2024
- Code Availability: https://github.com/ZhangLabGT/scDisInFact
  Description: A VAE that encodes shared-bio latent factors that capture biological
    variation (e.g. cell-type differences) and unshared-bio factors that capture condition-specific
    signals via separate encoders. Shared factors follow a standard normal prior,
    while unshared factors use a condition-specific Gaussian mixture prior. The invariance
    of the shared latent variables is enforced via an MMD penalty, while conditon-encoding
    in the unshared latent variables is promoted via a classification penalty. Group
    lasso is used to regularise condition-specific encoders, and it's (penalty) weights
    are used to select key genes per condition. scDisInFac enables perturbation predictions
    in multi-batch, multi-condition settings using scGEN-style arithmetics on the
    unshared space.
  Inspired by:
  - '-'
  Method: scDisInFac
  Model:
  - VAE
  - NB likelihood
  - Adversarial
  - Gaussian Mixture Model
  Publication: https://www.nature.com/articles/s41467-024-45227-w
  Published: true
  Task:
  - Contrastive Disentanglement
  - Nonlinear Gene Programmes
  - Seen Perturbation Prediction
  - Combinatorial Effect Prediction
  Year: 2024
- Code Availability: https://github.com/Vivianstats/scINSIGHT
  Description: "A non-negative matrix factorisation method that decomposes single-cell\
    \ gene expression data into common and condition-specific gene modulees. Each\
    \ sample\u2019s expression matrix is modeled as the sum of a shared component\
    \ (W\u2082V) and condition-specific components (W\u2081H\u2C7C), plus residual\
    \ noise. The approach minimizes a loss function combining reconstruction error\
    \ (Frobenius norm) with regularisation terms that control module scale and inter-condition\
    \ similarity."
  Inspired by:
  - '-'
  Method: scINSIGHT
  Model:
  - NMF
  Publication: https://link.springer.com/article/10.1186/s13059-022-02649-3#Sec11
  Published: true
  Task:
  - Contrastive Disentanglement
  - Linear Gene Programmes
  Year: 2022
- Code Availability: https://github.com/KlugerLab/SIMVI
  Description: "SIMVI is a spatially-informed VAE that disentangles gene expression\
    \ variability into two latent factors: an intrinsic variable z, which captures\
    \ cell type\u2013specific signals, and a spatial variable s, which quantifies\
    \ spatial effects. The spatial latent variable s is inferred by aggregating the\
    \ intrinsic representations of neighboring cells via a Graph Attention Network,\
    \ thereby incorporating local spatial context. To promote independence between\
    \ z and s, SIMVI employs an asymmetric regularisation on z using maximum mean\
    \ discrepancy or, alternatively, a  mutual information estimator, ensuring that\
    \ z retains minimal non-cell-intrinsic information. Furthermore, leveraging debiased\
    \ machine learning principles, the model decomposes gene expression variance by\
    \ treating s as a continuous treatment and z as confounding covariates, thereby\
    \ quantifying the specific impact of spatial context on gene expression."
  Inspired by:
  - Debiased ML
  - scVI
  Method: SIMVI
  Model:
  - ZINB Likelihood
  - VAE
  - Spatially-informed
  - Multi-modal
  Publication: https://www.nature.com/articles/s41467-025-58089-7
  Published: true
  Task:
  - Nonlinear Gene Programmes
  - Unsupervised Disentanglement
  Year: 2025
- Code Availability: github.com/theislab/trvae
  Description: 'trVAE enhances the scGEN model by incorporating condition embeddings
    and leveraging maximum mean discrepancy regularisation to manage distributions
    across binary conditions. By utilizing a conditional variational autoencoder,
    trVAE aims to create a compact and consistent representation of cross-condition
    distributions, enhancing out-of-distribution prediction accuracy. '
  Inspired by:
  - scGen
  Method: trVAE
  Model:
  - VAE
  Publication: https://academic.oup.com/bioinformatics/article/36/Supplement_2/i610/6055927#409207818
  Published: true
  Task:
  - Context Transfer
  - Seen Perturbation Prediction
  Year: 2020
- Code Availability: https://github.com/rampasek/DrVAE
  Description: Dr.VAE uses a Variational Autoencoder architecture to predict drug
    response from transcriptomic perturbation signatures. It models transcription
    change as a linear function within a low-dimensional latent space, defined by
    encoder and decoder neural networks. For paired expression samples from treated
    and control conditions, Dr.VAE accurately predicts treated expression.
  Inspired by:
  - '-'
  Method: Dr.VAE
  Model:
  - VAE
  Publication: https://academic.oup.com/bioinformatics/article/35/19/3743/5372343
  Published: true
  Task:
  - Context Transfer
  - Seen Perturbation Prediction
  Year: 2019
- Code Availability: https://github.com/theislab/scgen
  Description: 'scGen is VAE that uses latent space vector arithmetics to predict
    single-cell perturbation responses. The method first encodes high-dimensional
    gene expression profiles into a latent space, where it computes a difference vector
    (delta) representing the change between perturbed and unperturbed conditions.
    At inference, this delta vector is linearly added to the latent representation
    of unperturbed cells, and the adjusted latent vector is then decoded back into
    the original gene expression space, thereby simulating the perturbed state. '
  Inspired by:
  - Conditional VAE (CVAE)
  Method: scGEN
  Model:
  - VAE
  Publication: https://www.nature.com/articles/s41592-019-0494-8#Abs1
  Published: true
  Task:
  - Context Transfer
  Year: 2019
- Code Availability: https://github.com/sanderlab/CellBox
  Description: 'CellBox models cellular responses to perturbations, by linking molecular
    and phenotypic outcomes through a unified nonlinear ODE-based model, aimed at
    simulating dynamic cellular behavior. The framework uses gradient descent with
    automatic differentiation to infer ODE network interaction parameters, facilitating
    exposure to novel perturbations and prediction of cell responses. '
  Inspired by:
  - '-'
  Method: CellBox
  Model:
  - ODE
  Publication: https://www.cell.com/cell-systems/pdf/S2405-4712(20)30464-6.pdf
  Published: true
  Task:
  - Context Transfer
  - Seen Perturbation Prediction
  - Combinatorial Effect Prediction
  Year: 2021
- Code Availability: https://github.com/theislab/cpa
  Description: 'Compositional Perturbation Autoencoder (CPA) models single-cell gene
    expression under perturbations and covariates by decomposing expression into additive
    latent embeddings: a basal state, perturbation effects, and covariate effects.
    To ensure that the basal embedding is disentangled from perturbations and covariates,
    CPA employs an adversarial training scheme: auxiliary classifiers are trained
    to predict perturbations and covariates from the basal embedding, while the encoder
    is updated using a combined loss that discourages the basal representation from
    encoding such information. Perturbation embeddings are modulated by neural networks
    applied to continuous covariates (e.g., dose or time), enabling modeling of dose-response
    and combinatorial effects. '
  Inspired by:
  - scGEN
  - DANN
  - CVAE
  Method: CPA
  Model:
  - VAE
  - DANN-based Adversary that attempts to eliminate treatment effects/ cellular context
    from latent representation
  Publication: https://www.embopress.org/doi/full/10.15252/msb.202211517
  Published: true
  Task:
  - Context Transfer
  - Combinatorial Effect Prediction
  Year: 2023
- Code Availability: https://github.com/ JaneJiayiDong/scPreGAN
  Description: scPreGAN is a deep generative model that predicts the response of single-cell
    expression to perturbation by integrating an autoencoder and a generative adversarial
    network. The model extracts common information from unperturbed and perturbed
    data using an encoder network, and then generates perturbed data using a generator
    network. scPreGAN outperforms state-of-the-art methods on three real world datasets,
    capturing the complicated distribution of cell expression and generating prediction
    data with the same expression abundance as real data.
  Inspired by:
  - scGEN
  - CPA
  Method: scPreGan
  Model:
  - AE
  - GAN
  Publication: https://academic.oup.com/bioinformatics/article/38/13/3377/6593485
  Published: true
  Task:
  - Seen Perturbation Prediction
  Year: 2022
- Code Availability: github.com/theislab/chemCPA
  Description: A CPA extension that embeds prior knowlegde about the compound structure
    of drugs (SMILES representations), allowing it to extend CPA to unseen drug perturbations.
  Inspired by:
  - CPA (theirs)
  Method: ChemCPA
  Model:
  - AE framework inspired by CPA
  - Chemical representation embeddings
  Publication: https://proceedings.neurips.cc/paper_files/paper/2022/hash/aa933b5abc1be30baece1d230ec575a7-Abstract-Conference.html
  Published: true
  Task:
  - Unseen Perturbation Prediction
  - Context Transfer
  Year: 2022
- Code Availability: https://github.com/theislab/multicpa
  Description: MultiCPA extends CPA to predict combinatorial perturbation responses
    from CITE-seq data by integrating gene and protein modalities using either concatenation
    or a product of experts. It employs totalVI-inspired decoders and likelihoods
    to model modality-specific outputs.
  Inspired by:
  - CPA (theirs)
  Method: MultiCPA
  Model:
  - AE framework inspired by CPA
  - totalVI likelihood
  Publication: https://www.biorxiv.org/content/10.1101/2022.07.08.499049v1.abstract
  Published: true
  Task:
  - Combinatorial Effect Prediction
  Year: 2022
- Code Availability: https://github.com/broadinstitute/CellCap
  Description: CellCap is a deep generative model that extends CPA by incorporating
    cross-attention mechanisms between cell state and perturbation response (i.e.,
    its basal latent space and the perturbation design matrix). Further, CellCap uses
    a variational autoencoder (VAE) framework with a linear decoder to identify sparse
    and interpretable latent factors.
  Inspired by:
  - CPA
  - PerturbNet
  Method: CellCap
  Model:
  - VAE
  - Attention
  - Linear Decoder
  Publication: https://www.cell.com/cell-systems/fulltext/S2405-4712(25)00078-X
  Published: true
  Task:
  - Multi-component Disentanglement
  - Linear Gene Programmes
  Year: 2024
- Code Availability: https://github.com/snap-stanford/GEARS
  Description: GEARS is uses graph neural networks to learn multidimensional embeddings
    for genes and their perturbations by respectively leveraging gene co-expression
    and GO-derived similarity graphs. It first derives refined gene embeddings through
    a co-expression-based GNN and separately processes perturbation embeddings via
    a GO graph to incorporate prior biological relationships, with the latter design
    enabling predictions for unSeen Perturbation Prediction. These embeddings are
    integrated by adding the aggregated perturbation signal to the gene representations
    and then decoded using gene-specific layers augmented by a cross-gene context
    module, ultimately reconstructing the post-perturbation transcriptomic profile.
    The model is trained end-to-end with a combined autofocus and direction-aware
    loss, and it can optionally quantify uncertainty through a Gaussian likelihood
    framework.
  Inspired by:
  - '-'
  Method: GEARS
  Model:
  - GNNs for co-expression and GO relationships
  - Label embeddings
  - PK Representations
  Publication: https://www.nature.com/articles/s41587-023-01905-6#Abs1
  Published: true
  Task:
  - Combinatorial Effect Prediction
  - Unseen Perturbation Prediction
  Year: 2023
- Code Availability: https://github.com/BaiDing1234/AttentionPert
  Description: AttentionPert is a complex generative model that utilizes attention-based
    mechanisms to reconstruct perturbed cellular profiles from perturbation condition
    and precomputed Gene2Vec embeddings. It uses two encoders to capture global and
    local relationships between genes and perturbations (following GEARS). The PertWeight
    encoder models attention-based interactions between perturbations, while the PertLocal
    encoder identifies localized perturbation effects using an augmented GO graph.
  Inspired by:
  - GEARS
  Method: AttentionPert
  Model:
  - Transformer Model
  - GNN
  Publication: https://academic.oup.com/bioinformatics/article/40/Supplement_1/i453/7700899
  Published: true
  Task:
  - Combinatorial Effect Prediction
  - Unseen Perturbation Prediction
  Year: 2024
- Code Availability: https://github.com/Perturbation-Response-Prediction/PRnet
  Description: 'PRnet is a generative framework to predict the transcriptional response
    of cells to chemical perturbations. To learn the respose, the model randomly assigns
    control and perturbed cell pairs which are conditioned on the smiles embedding
    of the chemical perturbation and the dose. PRnet consists of three components:
    Perturb-adapter, Perturb-encoder, and Perturb-decoder, which work together to
    generate a distribution of transcriptional responses. Changing the smiles embedding
    can be used to predict the response of cells to novel chemical perturbations at
    both bulk and single-cell levels.'
  Inspired by:
  - '-'
  Method: PRNet
  Model:
  - DNN
  Publication: https://www.nature.com/articles/s41467-024-53457-1
  Published: true
  Task:
  - Unseen Perturbation Prediction
  Year: 2024
- Code Availability: https://github.com/sschrod/CODEX
  Description: CODEX uses a Deep Neural Network to map cells from control to perturbed
    states, learning perturbation effects in respective perturbation-dependent latent
    spaces. These latent spaces can be arbitrarily combined to infer unseen combinatorial
    effects, allowing the model to predict the outcomes of complex treatment combinations.
    Additionally, CODEX can leverage prior information from Gene Ontologies to inform
    the effects of completely unSeen Perturbation Prediction.
  Inspired by:
  - GEARS
  Method: CODEX
  Model:
  - DNN
  Publication: https://academic.oup.com/bioinformatics/article/40/Supplement_1/i91/7700898
  Published: true
  Task:
  - Combinatorial Effect Prediction
  - Unseen Perturbation Prediction
  - Context Transfer
  Year: 2024
- Code Availability: https://github.com/reem12345/Cell-Type-Specific-Graphs
  Description: PrePR-CT is a framework designed to predict transcriptional responses
    to chemical perturbations in unobserved cell types by utilizing cell-type-specific
    graphs encoded within Graph Attention Networks (GANs). The approach constructs
    cell graph priors using metacells which are randomly associated with perturbed
    cells to transform the problem into a regression task.
  Inspired by:
  - GEARS
  Method: PrePR-CT
  Model:
  - Graph attention
  - Regression
  Publication: https://www.biorxiv.org/content/10.1101/2024.07.24.604816v1.full.pdf
  Published: false
  Task:
  - Context Transfer
  Year: 2024
- Code Availability: https://github.com/mims-harvard/PDGrapher
  Description: 'PDGrapher builds on graph neural network (GNN) to predict therapeutic
    perturbations that can reverse disease phenotypes, focusing directly on identifying
    perturbation targets rather than modeling the perturbation effects. By embedding
    diseased cell states into gene regulatory networks or protein-protein interaction
    networks, PDGrapher learns latent representations to infer optimal perturbations
    that drive diseased states toward desired healthy outcomes. The method utilizes
    dual GNNs - a response prediction module and a perturbagen discovery module -
    both employing causal graphs as priors and adjusting edges to model interventions. '
  Inspired by:
  - GEARS
  Method: PDGrapher
  Model:
  - GNN
  Publication: https://pmc.ncbi.nlm.nih.gov/articles/PMC10802439/
  Published: false
  Task:
  - Combinatorial Effect Prediction
  - Unseen Perturbation Prediction
  - Context Transfer
  Year: 2025
- Code Availability: https://github.com/nitzanlab/biolord
  Description: 'A deep generative model that disentangles (multi-omics) single-cell
    data by separating sources of variation into known and unknown decomposed latent
    spaces, which are then concatenated for reconstruction. It requires partial supervision
    through known cell attributes, such as cell type, age, or perturbation, and employs
    different encoding strategies for categorical and continuous attributes. A contrastive
    objective maximizes reconstruction accuracy while minimizing information in unknown
    attributes, ensuring effective disentanglement. To further constrain the unknown
    latent space, Biolord uses activation penalty (L2) and Gaussian noise. '
  Inspired by:
  - LORD (Gabbay & Hoshen)
  Method: Biolord
  Model:
  - Probabilistic
  - ZINB likelihood
  - Protein-Count (totalVI) Likelihood
  Publication: https://www.nature.com/articles/s41587-023-02079-x#Sec6
  Published: true
  Task:
  - Multi-component Disentanglement
  - Unseen Perturbation Prediction
  - Combinatorial Effect Prediction
  - Context Transfer
  Year: 2024
- Code Availability: https://github.com/yulun-rayn/graphVCI
  Description: 'GraphVCI employs two parallel inference branches to estimate latent
    variables from factual and counterfactual inputs. In the factual branch, observed
    gene expressions, treatments, and covariates are encoded via an MLP combined with
    a GCN/GAT module that integrates a gene regulatory network; its corresponding
    decoder then reconstructs the observed expression profile. The sparse gene regulatory
    network is generated using a prior-informed drop out mechanism, based on ATAC-Seq
    data.  A parallel branch processes counterfactual treatments to generate alternative
    expression profiles. Training minimizes three losses: an individual-specific reconstruction
    loss computed as the negative log likelihood (e.g., under a normal or negative
    binomial distribution) of the observed expressions; a covariate-specific loss
    implemented as an adversarial network using a binary cross-entropy loss on the
    counterfactual outputs; and a KL divergence loss that regularizes and aligns the
    latent space between the factual and counterfactual branches.'
  Inspired by:
  - VCI (theirs)
  Method: graphVCI
  Model:
  - Dual-branch variational bayes causal inference framework
  - PK Representations
  Publication: https://openreview.net/pdf?id=ICYasJBlZNs
  Published: true
  Task:
  - Context Transfer
  Year: 2023
- Code Availability: https://github.com/hliulab/cycleCDR
  Description: cycle CDR uses a Cycle Consistent Learning strategy with a Complex
    AE architecture, consisting of two Encoder-Decoder pairs, to reconstruct control
    and perturbed samples. The two submodels are used in an alternating order to reconstruct
    the perturbed samples, and a GAN loss is applied to remove irrelevant information
    in the latent space. Additionally, chemical representations are added to the latent
    representation of each submodel to enhance the model's ability to capture chemical
    information.
  Inspired by:
  - chemCPA
  Method: cycleCDR
  Model:
  - Autoencoder
  Publication: https://academic.oup.com/bioinformatics/article/40/Supplement_1/i462/7700878
  Published: true
  Task:
  - Unseen Perturbation Prediction
  Year: 2024
- Code Availability: https://github.com/yulun-rayn/variational-causal-inference
  Description: GraphVCI predecessor, almost identical architecture, excluding the
    prior knowledge graphs.
  Inspired by:
  - '-'
  Method: VCI
  Model:
  - Dual-branch variational bayes causal inference framework
  Publication: https://arxiv.org/abs/2209.05935
  Published: false
  Task:
  - Context Transfer
  - Seen Perturbation Prediction
  Year: 2024
- Code Availability: '-'
  Description: SALT & PEPER represents a straightforward two-step approach. The initial
    SALT model assumes additive effects of individual perturbations. Building on this
    foundation, PEPER leverages a neural network to learn a non-linear correction,
    effectively accounting for non-additive combinatorial effects. Notably, despite
    its simplicity, this approach has demonstrated impressive performance on standard
    extrapolation benchmarks.
  Inspired by:
  - Linear Models
  Method: SALT&PEPER
  Model:
  - Additive Model
  - DNN
  Publication: https://arxiv.org/abs/2404.16907
  Published: true
  Task:
  - Combinatorial Effect Prediction
  Year: 2024
- Code Availability: https://github.com/siyuh/squidiff
  Description: Squidiff integrates a diffusion model with a variational autoencoder
    (VAE) to modulating cellular states and conditions using latent variables. Squidiff
    can accurately capture and reproduce cellular states, and can be used to generate
    new single-cell gene expression data over time and in response to stimuli
  Inspired by:
  - Diffusion
  Method: Squidiff
  Model:
  - Diffusion Model
  Publication: https://www.biorxiv.org/content/10.1101/2024.11.16.623974v1
  Published: false
  Task:
  - Combinatorial Effect Prediction
  - Context Transfer
  Year: 2024
- Code Availability: https://github.com/const-ae/pylemur
  Description: 'LEMUR is a PCA based algorithm that defines condition dependent embedings
    to analyze differences in differentialy expressed genes across conditions. For
    each condition a separate embeding matrix is learned and reconstructed using a
    shared matrix. This is used to generate counterfactual estimates for each cell
    and condition, which is used to infer label-free DE genes and neighborhoods. '
  Inspired by:
  - PCA
  Method: LEMUR
  Model:
  - Multi-condition PCA
  Publication: https://www.nature.com/articles/s41588-024-01996-0
  Published: true
  Task:
  - Seen Perturbation Prediction
  - Differential Analysis
  Year: 2025
- Code Availability: https://github.com/theislab/scarches
  Description: "Expimap uses a nonlinear encoder and a masked linear decoder, where\
    \ the latent space\u2019s dimensions are set equal to the number of gene programs,\
    \ and decoder weights are masked according to prior knowledge to ensure that each\
    \ latent variable reconstructs only genes associated with the geneset (fixed membership),\
    \ with L1 sparsity regularisation allowing soft membership for additional genes,\
    \ not included in the prior knowledge. Group lasso is additionally used to 'deactivate'\
    \ uniformative Gene Programmes."
  Inspired by:
  - Rybakov et al., 2020
  - VEGA
  - scVI
  - scArches (theirs)
  - oi-VAE
  - Conditional VAE
  Method: Expimap
  Model:
  - VAE
  - Linear Decoder
  - NB likelihood
  - PK Representations
  Publication: https://www.nature.com/articles/s41556-022-01072-x
  Published: true
  Task:
  - Nonlinear Gene Programmes
  Year: 2023
- Code Availability: 'https://github.com/ratschlab/pmvae '
  Description: "In pmVAE, each predefined pathway is modeled as a VAE that learns\
    \ a (local) multidimensional latent embedding for the genes in that pathway. Each\
    \ VAE module minimizes a size-weighted local reconstruction loss based solely\
    \ on its pathway\u2019s genes, while the (local) latent embeddings from all pathways\
    \ are concatenated to form a global representation. "
  Inspired by:
  - '-'
  Method: pmVAE
  Model:
  - Multiple VAEs
  - PK Representations
  Publication: https://icml-compbio.github.io/2021/papers/WCBICML2021_paper_24.pdf
  Published: true
  Task:
  - Linear Gene Programmes
  Year: 2021
- Code Availability: https://github.com/hdsu-bioquant/onto-vae
  Description: "ontoVAE uses a multi-layer, linear decoder, structured to represent\
    \ hierarchical prior knowledge - e.g. layers can represent gene ontology level.\
    \  To preserve connections beyond adjacent layers, the decoder concatenates outputs\
    \ from previous layers with the current layer\u2019s input, with binary masks\
    \ ensuring that only valid parent\u2013child and gene set relationships are captured.\
    \ Decoder weights are constrained to be positive to preserve directional pathway\
    \ activity, with each ontology term represented by three neurons whose average\
    \ activation reflects its activity."
  Inspired by:
  - DenseNet
  Method: ontoVAE
  Model:
  - VAE
  - Linear Decoder
  - PK Representations
  Publication: https://academic.oup.com/bioinformatics/article/39/6/btad387/7199588
  Published: true
  Task:
  - Linear Gene Programmes
  Year: 2023
- Code Availability: https://github.com/LucasESBS/vega/
  Description: VEGA replaces conventional fully connected decoder with a sparse linear
    decoder that uses a binary gene membership mask, assingning latent variables to
    a pre-defined collection of gene sets.
  Inspired by:
  - '-'
  Method: VEGA
  Model:
  - VAE
  - Linear Decoder
  - PK Representations
  Publication: https://www.nature.com/articles/s41467-021-26017-0
  Published: true
  Task:
  - Linear Gene Programmes
  Year: 2023
- Code Availability: https://github.com/Lotfollahi-lab/nichecompass.
  Description: "NicheCompass, the spatial sucessor of ExpiMap, employs multiple decoders:\
    \ one graph decoder reconstructs the spatial adjacency matrix via an adjacency\
    \ loss to ensure that spatially-neighboring observations have similar latent representations,\
    \ while separate (masked) decoders - one for each cell\u2019s own features and\
    \ one for its aggregated neighborhood features - reconstruct the omics data. By\
    \ masking the data reconstruction according to prior knowledge, each latent variable\
    \ is associated with a gene program (subclassified according inter- or  intracellular\
    \ signalling). Additionally, it learns de novo gene programs that capture novel,\
    \ spatially coherent expression patterns, not covered by the prior knowledge.\
    \ By default, it replaces the Group lasso loss of Expimap with a a dropout mechanism\
    \ to prune uninformative prior knowledge sets."
  Inspired by:
  - ExpiMap
  - GraphVAE
  Method: NicheCompass
  Model:
  - Graph VAE
  - Linear Decoder
  - NB Likelihood
  - Spatially-informed
  - PK Representations
  - Multi-modal
  Publication: https://www.nature.com/articles/s41588-025-02120-6
  Published: true
  Task:
  - Linear Gene Programmes
  Year: 2024
- Code Availability: https://github.com/namini94/EXPORT
  Description: 'EXPORT builds on the VEGA architecture by adding an auxiliary decoder
    that functions as an ordinal regressor, with an additional cumulative link loss
    to explicitly model dose-dependent response. '
  Inspired by:
  - VEGA
  - Cumulative link models
  Method: EXPORT
  Model:
  - VAE
  - Linear Decoder
  - PK Representations
  Publication: https://openreview.net/forum?id=f4nMJPKMkQ&referrer=%5Bthe%20profile%20of%20Xiaoning%20Qian%5D(%2Fprofile%3Fid%3D~Xiaoning_Qian1)
  Published: true
  Task:
  - Linear Gene Programmes
  Year: 2024
- Code Availability: https://github.com/MLO-lab/MuVI
  Description: "MuVI is a multi-view factor analysis that encodes prior knowledge\
    \ by imposing structured sparsity on view-specific factor loadings via a\
    \ weighted, regularized horseshoe prior. Specifically, it uses a weight parameter\
    \ that controls the variance of each loading; e.g., by default, it is set to 0.99\
    \ for genes known to belong to a gene set and 0.01 for genes which do not (are\
    \ uknown). Using this hieararchical regulairisation strategy, MuVI directly associates\
    \ latent factors with corresponding gene sets while still allowing for the de\
    \ novo identification of additional genes relevant to a given factor."
  Inspired by:
  - MOFA+
  Method: MuVi
  Model:
  - Group Factor Model
  - PK Representations
  - Multi-modal
  Publication: https://proceedings.mlr.press/v206/qoku23a.html
  Published: true
  Task:
  - Linear Gene Programmes
  Year: 2023
- Code Availability: https://github.com/hui2000ji/scETM
  Description: "scETM uses a standard VAE encoder with a softmax layer to obtain a\
    \ cell-by-topic matrix, paired with a linear decoder based on matrix tri-factorisation\
    \ that reconstructs the data from the cell-by-topic matrix, along with topics-by-embedding\
    \ \u03B1, and embedding-by-genes \u03C1 matrices. This structure allows the latent\
    \ topics to be directly interpreted as groups of co-expressed genes and can optionally\
    \ integrate prior pathway (prior knowledge) information as a binary mask."
  Inspired by:
  - LDA
  - ETM
  Method: scETM
  Model:
  - VAE
  - Embedding Topic Model
  - Linear Decoder
  - PK Representations
  Publication: https://www.nature.com/articles/s41467-021-25534-2
  Published: true
  Task:
  - Linear Gene Programmes
  Year: 2021
- Code Availability: https://github.com/dpeerlab/spectra
  Description: "Spectra decomposes a gene expression matrix into cell-by-\
    factor and factor-by-gene matrices, while integrating prior knowledge\
    \ gene sets and cell-type labels. It explicitly models both global and cell-type\u2013\
    specific factors by incorporating cell-type labels, thereby disentagling the typically\
    \ dominating cell-type variation from shared Gene Programmes. Gene sets are represented\
    \ as a gene\u2013gene knowledge graph, and a penalty term based on a weighted\
    \ Bernoulli likelihood, guides the factorisation toward preserving this graph.\
    \ Yet, it also permits the data-driven discovery of novel programs by 'detaching'\
    \ factors from the prior. Spectra can also include cell-type-specific prior knowledge\
    \ gene sets (e.g. T cell antigen receptor activation programmes can be limited\
    \ to T cells)"
  Inspired by:
  - '-'
  Method: Spectra
  Model:
  - Poisson Likelihood
  - Factor Analysis
  - PK Representations
  Publication: https://www.nature.com/articles/s41587-023-01940-3
  Published: true
  Task:
  - Linear Gene Programmes
  - Multi-component Disentanglement
  Year: 2023
- Code Availability: https://github.com/KANG-BIOINFO/CellDrift
  Description: CellDrift fits a negative binomial GLM to scRNA-seq counts using cell
    type, perturbation, and their interaction as independent (predictor) variables,
    while also incorporating library size and batch effects. Pairwise contrast coefficients
    are then derived to quantify the difference between perturbed and control states
    across time points. These time series of contrast coefficients, representing the
    temporal trajectory of perturbation effects per gene, are subsequently analyzed
    using Fuzzy C-means clustering to group similar temporal patterns and Functional
    PCA to extract the dominant modes of temporal variance.
  Inspired by:
  - '-'
  Method: CellDrift
  Model:
  - Generalised Linear Model
  - NB Likelihood
  - Functional PCA
  - Fuzzy Clustering
  - Time-resolved
  - Perturbation-Covariate Interactions
  Publication: https://academic.oup.com/bib/article/23/5/bbac324/6673850#373524408
  Published: true
  Task:
  - Differential Analysis
  Year: 2022
- Code Availability: https://github.com/vandijklab/CINEMA-OT
  Description: "CINEMA-OT disentangles perturbation effects from confounding\
    \ variation by decomposing the data with independent component analysis (ICA);\
    \ ICA components correlated with the perturbation labels are identified using\
    \ Chatterjee\u2019s coefficient and excluded, yielding a background (confounder)\
    \ latent space that predominantly reflects confounding factors. Optimal transport\
    \ is then applied to this background space to align perturbed and control cells,\
    \ thereby generating counterfactual cell pairs, and this OT map is used in downstream\
    \ analyses. They also propose a reweighting variant (CINEMA-OT-W) to\
    \ address differential cell type abundance by pre-aligning treated cells with\
    \ k-nearest neighbor controls and balancing clusters prior to ICA and optimal\
    \ transport."
  Inspired by:
  - Mixscape
  - OTT
  Method: CINEMA-OT
  Model:
  - Unbalanced OT
  - Entropy-regularized Sinkhorn
  - ICA
  Publication: https://www.nature.com/articles/s41592-023-02040-5#Sec11
  Published: true
  Task:
  - Trace Cell Populations
  - Perturbation Responsiveness
  - Unsupervised Disentanglement
  Year: 2023
- Code Availability: https://github.com/bunnech/cellot
  Description: "CellOT learns mappings between control and perturbed cell state distributions\
    \ by solving a dual formulation of the optimal transport problem. The approach\
    \ learns optimal transport maps as the gradient of a convex potential function,\
    \ which is approximated using input convex neural networks - (briefly) a specific\
    \ type of neural network with convex-preserving constraints, such as non-negative\
    \ weights and a predefined set of activation functions (e.g. ReLU). Instead of\
    \ relying on regularisation-based OT (e.g. Entropy-regularised Sinkhorn), it jointly\
    \ optimizes dual potentials (a pair of functions) via a max\u2013min loss."
  Inspired by:
  - Makkuva et al, 2020
  Method: CellOT
  Model:
  - Dual (min-max) Formulation OT
  Publication: https://www.nature.com/articles/s41592-023-01969-x
  Published: true
  Task:
  - Trace Cell Populations
  - Perturbation Responsiveness
  - Context Transfer
  Year: 2023
- Code Availability: https://github.com/bunnech/condot/tree/main
  Description: 'CondOT builds on CellOT to learn context-aware optimal transport maps
    by conditioning on an auxiliary variable. Instead of learning a fixed transport
    map, it learns a context-dependent transport map that adapts based on this auxiliary
    information. The OT map is modeled as the gradient of a convex potential using
    partially input convex neural networks, which ensures mathematical properties
    required for parametrised optimal transport. The auxiliary variables can be of
    different types: continuous (like dosage or spatial coordinates), categorical
    (like treatment groups, represented via one-hot encoding), or learned embeddings
    learned. Additionally, CondOT includes a separate neural module, a combinator
    network, for combinatorial predictions.'
  Inspired by:
  - Amos et al., 2017
  - Makkuva et al., 2020
  - CellOT (theirs)
  Method: CondOT
  Model:
  - Conditioned Dual (min-max) Formulation OT
  Publication: https://proceedings.neurips.cc/paper_files/paper/2022/file/2d880acd7b31e25d45097455c8e8257f-Paper-Conference.pdf
  Published: true
  Task:
  - Trace Cell Populations
  - Unseen Perturbation Prediction
  - Combinatorial Effect Prediction
  - Context Transfer
  Year: 2022
- Code Availability: https://genentech.github.io/Perturb-OT/
  Description: The paper extends entropic Gromov-Wasserstein Optimal Transport and
    Co-Optimal Transport to incorporate perturbation labels for aligning data across
    different modalities from large-scale perturbation screens. The core innovation
    involves constraining the learned cross-modality coupling matrix to be "label-compatible",
    meaning that the transport plan is informed by the perturbation labels and is
    only allowed to match between cells that have received the same perturbation label,
    which is achieved by modifying the Sinkhorn algorithm. This label-compatible alignment
    is used to train a model to estimate cellular responses to perturbations when
    measurements are available in only one modality.
  Inspired by:
  - '-'
  Method: GWOT
  Model:
  - Optimal Transport
  - Multi-modal
  Publication: https://arxiv.org/pdf/2405.00838
  Published: true
  Task:
  - Trace Cell Populations
  - Context Transfer
  Year: 2025
- Code Availability: github.com/Genentech/MMFM
  Description: MMFM (Multi-Marginal Flow Matching) builds on Flow Matching to model
    cell trajectories across time and conditions. MMFM generalizes the Conditional
    Flow Matching framework to incorporate multiple time points using a spline-based
    conditional probability path. Moreover, it leverages ideas from classifier-free
    guidance to incorporate multiple conditions.
  Inspired by:
  - Conditional Flow Matching
  - Optimal Transport
  Method: MMFM
  Model:
  - Flow Matching
  - Optimal Transport
  Publication: https://openreview.net/pdf?id=hwnObmOTrV
  Published: true
  Task:
  - Trace Cell Populations
  - Context Transfer
  - Seen Perturbation Prediction
  Year: 2024
- Code Availability: https://github.com/kksniak/metric-flow-matching.git
  Description: Metric Flow Matching (MFM) constructs probability paths between source
    and target distributions by interpolating geodesics following a data-dependent
    Riemannian metric, ensuring that interpolations remain close to the data manifold
    rather than being straight lines in Euclidean space. MFM first learns these geodesics
    by minimizing a special cost function, and then regresses a vector field along
    a geodesic-based corrected path using a conditional flow matching objective.
  Inspired by:
  - Conditional Flow Matching
  - Optimal Transport
  Method: MFM
  Model:
  - Flow Matching
  - Optimal Transport
  Publication: https://proceedings.neurips.cc/paper_files/paper/2024/file/f381114cf5aba4e45552869863deaaa7-Paper-Conference.pdf
  Published: true
  Task:
  - Trace Cell Populations
  - Context Transfer
  - Seen Perturbation Prediction
  Year: 2024
- Code Availability: https://github.com/EperLuo/scDiffusion
  Description: scDiffusion employs a Latent Diffusion Model for generating single-cell
    RNA sequencing data, using a three-part framework - a fine-tuned autoencoder for
    initial data transformation, a skip-connected multilayer perceptron denoising
    network, and a condition controller for cell-type-specific data generation.
  Inspired by:
  - '-'
  Method: scDiffusion
  Model:
  - Diffusion
  - VAE
  Publication: https://academic.oup.com/bioinformatics/article/40/9/btae518/7738782
  Published: true
  Task:
  - Context Transfer
  Year: 2024
- Code Availability: https://github.com/theislab/CFGen
  Description: CFGen is a flow-based model for producing multi-modal scRNA-seq data.
    CFGen builds on CellFlow and explicitly models the discrete, over-dispersed nature
    of single-cell counts when generating synthetic data.
  Inspired by:
  - CellFlow
  Method: CFGen
  Model:
  - Optimal Transport
  - Multi-modal
  - Conditional Flow Matching
  Publication: https://openreview.net/forum?id=3MnMGLctKb
  Published: true
  Task:
  - Trace Cell Populations
  - Context Transfer
  Year: 2024
- Code Availability: '-'
  Description: CellFlow learns a vector field to predict time-dependent expression
    profiles under diverse conditions. The model encodes various covariates (perturbation,
    dosage, batch, etc.), aggregates the embeddings via attention and deep sets, and
    uses a conditional flow matching framework to learn the underlying flow of the
    effect.
  Inspired by:
  - CellOT
  Method: cellFlow
  Model:
  - Conditional Flow Matching
  - Optimal Transport
  Publication: https://www.biorxiv.org/content/10.1101/2025.04.11.648220v1.full.pdf
  Published: false
  Task:
  - Trace Cell Populations
  - Context Transfer
  - Unseen Perturbation Prediction
  - Combinatorial Effect Prediction
  Year: 2024
- Code Availability: https://github.com/broadinstitute/wot
  Description: "Waddington-OT models developmental processes as time-varying
    \ probability distributions in gene expression space and infers temporal couplings\
    \ by solving an entropy-regularized, unbalanced optimal transport problem.\
    \ Growth rate, estimated leveraging expression levels of genes associated with\
    \ proliferation and apoptosis, is taken into consideration via unbalanced OT.\
    \ Additionally, uses spectral clustering to obtain Gene Programmes, and subsequently\
    \ associate those to predictive TFs."
  Inspired by:
  - '-'
  Method: Waddington-OT
  Model:
  - Unbalanced OT
  - "Entropy-regularized Sinkhorn"
  Publication: https://www.sciencedirect.com/science/article/pii/S009286741930039X?via%3Dihub
  Published: true
  Task:
  - Trace Cell Populations
  - Linear Gene Programmes
  Year: 2019
- Code Availability: https://github.com/atong01/conditional-flow-matching
  Description: "The paper introduces two key Flow Matching variants: (i) Optimal Transport\
    \ CFM (OT-CFM), which uses optimal transport couplings (approximated with minibatch\
    \ OT), to produce more robust flow inference and (ii) Schr\xF6dinger Bridge CFM\
    \ solving the Schr\xF6dinger Bridge problem by using entropy-regularized OT couplings. "
  Inspired by:
  - Flow Matching
  Method: OT-CFM
  Model:
  - Flow Matching
  - "Schr\xF6dinger Bridge"
  Publication: https://openreview.net/forum?id=CD9Snc73AW
  Published: true
  Task:
  - Trace Cell Populations
  Year: 2024
- Code Availability: 'https://github.com/vsomnath/aligned_diffusion_bridges '
  Description: "SBALIGN solves a Diffusion Schr\xF6dinger Bridge Problem with aligned\
    \ data, i.e. where each sample from the source distribution is paired with a corresponding\
    \ sample from the target distribution. It combines classical SB theory with Doob's\
    \ h-transform to derive a novel loss -- parameterizing the drift and h-function\
    \ with neural networks, enabling more stable training than iterative procedures."
  Inspired by:
  - "Schr\xF6dinger Bridges"
  Method: SBALIGN
  Model:
  - "Schr\xF6dinger Bridge"
  Publication: https://proceedings.mlr.press/v216/somnath23a/somnath23a.pdf
  Published: true
  Task:
  - Trace Cell Populations
  Year: 2023
- Code Availability: https://github.com/KrishnaswamyLab/MIOFlow
  Description: MIOFlow learns (stochastic) continuous dynamics from snapshot data
    by combining neural ODEs, manifold learning, and optimal transport. The method
    first trains a Geodesic Autoencoder to embed data into a latent space where geodesic-based
    distances are preserved, then models trajectories via a neural ODE and an ODE
    solver, and optimizes the Wasserstein loss between actual and predicted distributions.
  Inspired by:
  - '-'
  Method: MioFlow
  Model:
  - Neural ODE
  - Geodasic Autoencoder
  Publication: https://proceedings.neurips.cc/paper_files/paper/2022/file/bfc03f077688d8885c0a9389d77616d0-Paper-Conference.pdf
  Published: true
  Task:
  - Trace Cell Populations
  Year: 2022
- Code Availability: https://github.com/theislab/moscot
  Description: "Moscot is a broad and scalable framework that recasts various single-cell\
    \ mapping tasks as optimal transport problems, supporting formulations that compare\
    \ distributions in shared (Wasserstein-type OT), distinct (Gromov-Wasserstein\
    \ OT), and partially-overlapping feature spaces (fused-Gromov\u2013Wasserstein\
    \ OT). Beyond Entropy-regularized sinkhorn (Cuturi et al., 2013), moscot provides\
    \ a user-friendly API to more recent OT strategies, such as low-rank and sparse\
    \ Monge maps."
  Inspired by:
  - Waddington-OT
  - NovoSpaRc
  - PASTE
  - OTT
  Method: moscot
  Model:
  - Unbalanced OT
  - Entropy-regularised Sinkhorn
  - Low-rank OT
  - Sparse Map OT
  Publication: https://www.nature.com/articles/s41586-024-08453-2
  Published: true
  Task:
  - Trace Cell Populations
  Year: 2025
- Code Availability: https://github.com/jkobject/geneformer
  Description: "Geneformer is a context-aware transformer encoder comprising six layers\
    \ of full dense self-attention over an input sequence of up to 2,048 genes, producing\
    \ embeddings for genes and cells. Genes in each single-cell transcriptome are\
    \ encoded as  rank value vectors - each gene\u2019s expression is ranked within\
    \ each cell. Pretraining uses a self-supervised masked learning objective (masking\
    \ 15% of gene tokens and minimizing a prediction loss to recover their identities)."
  Inspired by:
  - '-'
  Method: Geneformer
  Model:
  - Foundational Gene expression embeddings (from ~30M human cells)
  - Self-supervised masked regression
  - Standard transformer attention
  Publication: https://www.nature.com/articles/s41586-023-06139-9
  Published: true
  Task:
  - GRN Inference
  Year: 2023
- Code Availability: https://github.com/bowang-lab/scGPT
  Description: scGPT processes each cell as a sequence of gene tokens, expression-value
    tokens and condition tokens (e.g., batch, perturbation or modality), embedding
    each and summing before feeding them into stacked transformer blocks whose specialised,
    masked multi-head attention layers enable autoregressive prediction of masked
    gene expressions from non-sequential data. scGPT is pretrained using a masked
    gene expression-prediction objective that jointly optimizes cell and gene embeddings,
    and can be fine-tuned on smaller datasets with task-specific supervised losses.
    For gene regulatory network inference, scGPT derives k-nearest neighbor similarity
    graphs from learned gene embeddings and analyses attention maps to extract context-specific
    Gene Programmes and gene-gene interactions.
  Inspired by:
  - GPT series
  Method: scGPT
  Model:
  - Foundational Gene expression embeddings (from >33M human cells)
  - Self-supervised masked expression prediction
  - Customised non-sequential (flash) attention
  Publication: https://www.nature.com/articles/s41592-024-02201-0
  Published: true
  Task:
  - Unseen Perturbation Prediction
  - Combinatorial Effect Prediction
  - GRN Inference
  Year: 2024
- Code Availability: https://github.com/HelloWorldLTY/scELMo
  Description: "scELMo first converts gene and cell metadata into textual descriptions\
    \ and uses GPT-3.5 to generate fixed-length embeddings, which are integrated with\
    \ normalised expression values by arithmetic or weighted averaging in a zero-shot\
    \ framework to yield cell embeddings. For some tasks, these embeddings and are\
    \ fine-tuned via a compact neural adaptor trained with combined classification\
    \ and contrastive losses. These embeddings are also fed into CPA\u2019s conditional\
    \ variational autoencoder and GEARS\u2019s graph neural network for perturbation\
    \ response prediction"
  Inspired by:
  - GenePT
  Method: scELMo
  Model:
  - Converts gene/cell metadata into text embeddings
  - 'Integrates text and expression embeddings '
  - Fine-tunes embeddings via a lightweight neural adaptor
  Publication: https://www.biorxiv.org/content/10.1101/2023.12.07.569910v2
  Published: true
  Task:
  - Seen Perturbation Prediction
  Year: 2024
- Code Availability: https://github.com/vandijklab/cell2sentence
  Description: "C2S-Scale is a family of large language models (LLMs) for single-cell\
    \ RNA-seq analysis that extends the Cell2Sentence (C2S) framework by converting\
    \ cell gene-expression profiles into ordered \u201Ccell sentences\u201D for natural-language\
    \ processing.Each C2S-Scale model is initialized from a publicly released Gemma-2\
    \ or Pythia checkpoint, i.e. leverages pre-existing language representations,\
    \ and is then further pre-trained on a multimodal corpus of over a billion tokens.\
    \ Each cell sentence is paired with the abstract (and, where available, additional\
    \ free-text annotations) from the same study, allowing the model to learn matched\
    \ transcriptomic and experimental context. "
  Inspired by:
  - Cell2Sentence (theirs)
  Method: C2S-Scale
  Model:
  - Family of LLMs with to 27B parameters
  Publication: https://www.biorxiv.org/content/10.1101/2025.04.14.648850v1
  Published: false
  Task:
  - Unseen Perturbation Prediction
  - Combinatorial Effect Prediction
  Year: 2025
- Code Availability: '-'
  Description: LPM is a decoder-only deep neural network designed for large-scale
    integration and prediction across heterogeneous perturbation datasets. LPM encodes
    perturbation (P), readout (R), and context (C) as discrete variables, each with
    its own embedding space implemented via learnable look-up tables. These embeddings
    are concatenated to and used for inference
  Inspired by:
  - '-'
  Method: LPM
  Model:
  - DNN Decoder
  Publication: https://arxiv.org/pdf/2503.23535
  Published: false
  Task:
  - Unseen Perturbation Prediction
  - Combinatorial Effect Prediction
  - GRN Inference
  Year: 2025
- Code Availability: '-'
  Description: "scGenePT combines CRISPR single-cell RNA-seq perturbation\
    \ data with language-based gene embeddings. It builds on a pretrained scGPT\
    \ by adding gene-level text embeddings from NCBI Gene/UniProt summaries or\
    \ GO annotations, to the token, count, and perturbation embeddings of the model\
    \ during fine-tuning on perturbational data."
  Inspired by:
  - scGPT
  - GenePT
  Method: scGenePT
  Model:
  - scGPT
  - ChatGPT prompts
  Publication: https://www.biorxiv.org/content/10.1101/2024.10.23.619972v1
  Published: false
  Task:
  - Unseen Perturbation Prediction
  - Combinatorial Effect Prediction
  - GRN Inference
  Year: 2025
- Code Availability: https://github.com/biomap-research/scFoundation
  Description: "scFoundation uses an asymmetric transformer encoder\u2013decoder:\
    \ its embedding module converts each continuous gene expression scalar directly\
    \ into a high-dimensional learnable vector without discretisation; the encoder\
    \ takes as input only nonzero and unmasked embeddings through vanilla transformer\
    \ blocks to model gene\u2013gene dependencies efficiently. The zero and masked\
    \ gene embeddings, along with the encoder embeddings, are passed to the decoder,\
    \ which uses Performer-style attention to reconstruct transcriptome-wide representations,\
    \ specifically those of masked genes. Specifically, scFoundation is trained using\
    \ a masked regression objective on both raw and downsampled count vectors, with\
    \ two total-count tokens concatenated to inputs to account for sequencing depth\
    \ variance. The decoder-derived gene context embeddings are then used as node\
    \ features in GEARS for single-cell perturbation response prediction."
  Inspired by:
  - 'xTrimoGene '
  - Performer
  Method: scFoundation
  Model:
  - Foundational Gene expression embeddings (from >50M human cells)
  - Self-supervised masked regression with down-sampling
  - Sparse transformer encoder
  - Performer-style attention decoder
  Publication: https://www.nature.com/articles/s41592-024-02305-7
  Published: true
  Task:
  - Nonlinear Gene Programmes
  - Unseen Perturbation Prediction
  - Combinatorial Effect Prediction
  Year: 2024
- Code Availability: https://github.com/xCompass-AI/GeneCompass
  Description: "GeneCompass is a knowledge-informed, cross-species foundation model.\
    \ During pre-training it integrates four types of prior biological knowledge -\
    \ gene regulatory networks (ENCODE PECA2-derived GRNs), promoter sequences\
    \ (fine-tuned DNABert embeddings), gene family annotations (gene2vec HGNC/esnembl\
    \ embeddings), and gene co-expression relationships (Pearson Correlations in their\
    \ dataset) - into a unified embedding space. It employs a masked-language-modeling\
    \ strategy by randomly masking 15 % of gene inputs and simultaneously reconstructs\
    \ both gene identities and expression values; this is optimized via a multi-task\
    \ loss combining mean squared error for expression recovery and cross-entropy\
    \ for gene ID prediction, balanced by a weighting hyperparameter \u03B2. Combined\
    \ with GEARS for extrapolation tasks."
  Inspired by:
  - Geneformer
  Method: GeneCompass
  Model:
  - Foundational Gene expression embeddings (from >50M human cells)
  - Self-supervised masked regression with down-sampling
  - Sparse transformer encoder
  - Performer-style attention decoder
  - PK-informed
  Publication: https://www.nature.com/articles/s41422-024-01034-y
  Published: true
  Task:
  - Unseen Perturbation Prediction
  - Combinatorial Effect Prediction
  - GRN Inference
  Year: 2024
- Code Availability: https://github.com/cantinilab/scPRINT
  Description: "scPRINT is implemented as a bidirectional transformer, focusing on\
    \ scalable zero-shot applications to new datasets. During pre-training, it optimises\
    \ a single composite loss that sums: (1) a denoising objective, which up-samples\
    \ down-sampled transcript counts via a zero-inflated negative-binomial decoder;\
    \ (2) a bottleneck reconstruction objective, where the model must regenerate full\
    \ expression profiles from its compressed cell embedding; and (3) a hierarchical\
    \ label-prediction objective that forces disentanglement of latent factors for\
    \ cell type, disease, platform and other metadata. Each gene token is the sum\
    \ of: a learned protein embedding for its gene ID; an MLP encoding of its log-normalized\
    \ count; and a positional encoding of its genomic locus . Pre-training contexts\
    \ consist of 2,200 randomly sampled expressed genes per cell. At inference, cell-specific\
    \ gene networks are derived from the model\u2019s multi-head attention maps by\
    \ either averaging all heads or selecting a subset post hoc based on correlation\
    \ with external priors (e.g., protein\u2013protein interaction databases, ChIP-seq,\
    \ perturbation-ground-truth networks)."
  Inspired by:
  - BERT
  - scVI
  - UCE
  Method: scPrint
  Model:
  - Foundational Gene expression embeddings (from >50M human cells)
  - BERT-like Bidirectional transformers (with flashattention2)
  - Self-supervised masked regression
  - A classifier decoder
  - ZINB likelihood decoder
  - PK Representations
  Publication: https://www.nature.com/articles/s41467-025-58699-1
  Published: true
  Task:
  - GRN Inference
  - Multi-component Disentanglement
  Year: 2025
- Code Availability: '-'
  Description: 'The method embeds each gene using two LLM-derived representations
    - GPT-3.5 text embeddings of NCBI gene descriptions and ProtT5 protein sequence
    embeddings; and, after reducing them to the top 50 principal components, uses
    these as inputs to a multi-output Gaussian Process regression model with an RBF
    kernel to predict the differential expression response to single-gene knockouts. '
  Inspired by:
  - GenePT
  Method: LLM+GP
  Model:
  - Gaussian Process Model
  - Language embeddings
  Publication: https://openreview.net/forum?id=eb3ndUlkt4
  Published: true
  Task:
  - Unseen Perturbation Prediction
  Year: 2024
- Code Availability: '-'
  Description: TODO
  Inspired by:
  - Kuipers et al (2022)
  Method: CIV
  Model:
  - Active Learning
  - Structural Causal Model
  - DAG-Bayesian linear regression
  Publication: https://www.nature.com/articles/s42256-023-00719-0
  Published: true
  Task:
  - Causal Structure
  Year: 2023
- Code Availability: https://github.com/Durenlab/LINGER
  Description: TODO
  Inspired by:
  - '-'
  Method: LINGER
  Model:
  - Multi-modal
  - Prior Knowledge Informed
  - Shapley values
  - DNN
  Publication: https://www.nature.com/articles/s41587-024-02182-7
  Published: true
  Task:
  - GRN Inference
  Year: 2024
- Code Availability: https://github.com/aertslab/scenicplus
  Description: TODO
  Inspired by:
  - SCENIC/GENIE3(theirs)
  Method: SCENIC+
  Model:
  - Multi-modal
  - Prior Knowledge Informed
  - Gradient Boosting
  Publication: https://www.nature.com/articles/s41592-023-01938-4
  Published: true
  Task:
  - GRN Inference
  Year: 2022
- Code Availability: https://github.com/morris-lab/CellOracle
  Description: TODO
  Inspired by:
  - '-'
  Method: CellOracle
  Model:
  - Multi-modal
  - Prior Knowledge Informed
  - Regularised (Linear) Regression
  Publication: https://www.nature.com/articles/s41586-022-05688-9
  Published: true
  Task:
  - GRN Inference
  Year: 2023
- Code Availability: https://github.com/xunzheng/notears
  Description: 'NOTEARS replaced traditional statistical DAG learning techniques for
    observational data with a continuous optimisation problem, by reformulating the
    acyclicity constraint. This reduces the computational complexity and facilitated
    first small scale biological applications. '
  Inspired by:
  - '-'
  Method: NOTEARS
  Model:
  - Continuous optimisation for acyclicity
  Publication: https://arxiv.org/abs/1803.01422
  Published: true
  Task:
  - Causal Structure
  Year: 2018
- Code Availability: https://github.com/xunzheng/notears
  Description: NOTEARS-MLP  further generalized the continuous DAG objective introduced
    by NOTEARS to nonparametric and semi-parametric models, such as deep neural networks
    (DNNs), to better facilitate non-linear data.
  Inspired by:
  - NOTEARS
  Method: NOTEARS-MLP
  Model:
  - Continuous optimisation for acyclicity
  - DNN
  Publication: https://arxiv.org/abs/1909.13189
  Published: true
  Task:
  - Causal Structure
  Year: 2020
- Code Availability: https://github.com/fishmoon1234/DAG-GNN
  Description: 'DAG-GNN introduced a polynomial alternative for the acyclicity constraint
    of NOTEARS, and encodes the DAG in a Graph Neural Network. Experimental results
    on synthetic data sets indicate that DAG-GNN learns more accurate graphs for non-linearly
    generated samples. '
  Inspired by:
  - NOTEARS
  Method: DAG-GNN
  Model:
  - Continuous optimisation for acyclicity
  - GNN
  Publication: https://arxiv.org/abs/1904.10098
  Published: true
  Task:
  - Causal Structure
  Year: 2019
- Code Availability: https://github.com/slachapelle/dcdi
  Description: DCDI advanced DAG learning by introducing a framework for causal discovery
    using interventional data. DCDI encoding interventions using a binary adjacency
    matrix, to replicate the interventional effects directly the DAG and uses neural
    networks to model the conditional densities. Further, the authors provided theoretical
    guarantees for DAG learning using interventional data and showed that the inferred
    graphs can scale to 100 nodes.
  Inspired by:
  - NOTEARS
  - Normalizing Flows
  Method: DCDI
  Model:
  - Graph interventions
  - DNN
  - Normalizing-Flows
  Publication: https://arxiv.org/pdf/2007.01754
  Published: true
  Task:
  - Causal Structure
  Year: 2020
- Code Availability: https://github.com/Genentech/nodags-flows
  Description: NODAGS-Flow utilizes contractive residual flows to model perturbational
    data as generated from the steady state of a dynamical system with explicit noise.
    Following DCDI, NODAGS-Flow replicates perturbations on the graph. Further, NODAGS-Flow
    drops the acyclicity constraint to model cyclic causal models and better explain
    the feedback loops inherent to biological data.
  Inspired by:
  - NOTEARS
  - DCDI
  Method: NODAGS-Flow
  Model:
  - Graph interventions
  - DNN
  - 'Residual Flow

    Steady-State ODE'
  Publication: https://proceedings.mlr.press/v206/sethuraman23a/sethuraman23a.pdf
  Published: true
  Task:
  - Causal Structure
  Year: 2023
- Code Availability: https://github.com/PMBio/bicycle
  Description: Bicycle addresses the challenge of robustly identifying cyclic causal
    graphs, particularly in domains like single-cell genomics, by leveraging perturbation
    data and explicitly replicating the perturbations on the graph. Following Dictys
    Bicycle assumes the perturbed cell states to be the steady-state solution of the
    Ornstein-Uhlenbeck process.
  Inspired by:
  - Dictys
  - NODAGS-Flow
  Method: Bicycle
  Model:
  - Graph interventions
  - Ornstein-Uhlenbeck process
  - Steady-State ODE
  Publication: https://proceedings.mlr.press/v236/rohbeck24a.html
  Published: true
  Task:
  - Causal Structure
  Year: 2023
- Code Availability: https://github.com/uhlerlab/discrepancy_vae
  Description: "A VAE that disentangles control and pertubed cells into a latent space\
    \ organized by a causal DAG. The encoder produces a Gaussian latent code z, while\
    \ an intervention encoder transforms intervention one-hot encodings into two embeddings\
    \ - a soft assignment vector that targets specific latent dimensions and a scalar\
    \ capturing the intervention\u2019s magnitude. Multiplying and adding these embeddings\
    \ to z yields a modified latent vector that simulates a soft intervention, whereas\
    \ zeroing them recovers the control condition. A causal layer then processes the\
    \ latent vectors using an upper-triangular matrix G, which enforces an acyclic\
    \ causal structure and propagates intervention effects among the latent factors.\
    \ The decoder is applied twice - once to the modified latent code to generate\
    \ virtual counterfactual outputs that reconstruct interventional outcomes, and\
    \ once to the unmodified code to recover control samples. This dual decoding forces\
    \ the model to disentangle intervention-specific effects from the intrinsic data\
    \ distribution. The training objective combines reconstruction error to reconstruct\
    \ control samples, a discrepancy loss (e.g., MMD) to align virtual counterfactuals\
    \ with observed interventional data, KL divergence on the latent space, and an\
    \ L1 penalty on G to enforce sparsity."
  Inspired by:
  - DSCM
  Method: discrepancy-VAE
  Model:
  - VAE
  - Disentanglement via Virtual Counterfactuals
  Publication: https://openreview.net/forum?id=o16sYKHk3S&noteId=2EQ6cmfPHg
  Published: true
  Task:
  - Multi-component Disentanglement
  - Seen Perturbation Prediction
  - Combinatorial Effect Prediction
  - Causal Structure
  Year: 2023
- Code Availability: https://github.com/Genentech/dcdfg
  Description: 'DCD-FG leverages a Gaussian low-rank structural equation model to
    model factor directed acyclic graphs (f-DAGs). The f-DAG assumption posits that
    many nodes share a similar set of parents and children, reflecting the behavior
    of genes acting collectively in biological programs. This method restricts the
    search space to low-rank causal interactions to improve causal discovery accuracy
    and scalability for high-dimensional data. '
  Inspired by:
  - DCDI
  - NOTEARS
  Method: DCD-FG
  Model:
  - Factor Model
  - DAGs
  - Latent DAGs
  Publication: https://proceedings.neurips.cc/paper_files/paper/2022/file/7a8fa1382ea068f3f402b72081df16be-Paper-Conference.pdf
  Published: true
  Task:
  - Causal Structure
  Year: 2022
- Code Availability: https://github.com/pinellolab/dictys
  Description: Dictys integrates scRNA-seq and scATAC-seq data to infer gene regulatory
    networks (GRNs) and their changes across multiple conditions. By leveraging multiomic
    data, Dictys infers context-specific networks and dynamic GRNs using steady-state
    solutions of the Ornstein-Uhlenbeck process to model transcriptional kinetics
    and account for feedback loops. It reconstructs GRNs by detecting transcription
    factor (TF) binding sites and refining these networks with single-cell transcriptomic
    data, capturing regulatory shifts that reflect TF activity beyond expression levels.
  Inspired by:
  - '-'
  Method: Dictys
  Model:
  - "Ornstein\u2013Uhlenbeck process"
  - Steady-State ODE
  Publication: https://www.nature.com/articles/s41592-023-01971-3
  Published: true
  Task:
  - GRN Inference
  - Causal Structure
  Year: 2023
- Code Availability: https://github.com/larslorch/avici
  Description: AVICI proposes an amortized causal discovery approach, attempting to
    directly predict causal structures from observational or interventional data using
    variational inference rather than performing costly searches over possible structures.
    Since no ground truth is not available for real data, the mode is pre-trained
    using simulated data with known causal graphs and subsequently applied to real
    data.
  Inspired by:
  - '-'
  Method: AVICI
  Model:
  - Amortized pre-training
  - 'Variational Inference '
  Publication: https://arxiv.org/abs/2205.12934
  Published: true
  Task:
  - Causal Structure
  Year: 2022
- Code Availability: http://uhlerlab.github.io/causaldag/dci
  Description: DCI introduced a reformulated version of the PC algorithm. Rather than
    inferring the Causal Graph directly DCI attempts to identify causal differences
    between condition-dependent gene regulatory networks (GRNs) by focusing on edges
    that appear, disappear, or change between conditions. This significantly reduces
    the computational complexity in comparison to the original PC algorithm.
  Inspired by:
  - PC Algorithm
  Method: DCI
  Model:
  - PC Algorithm
  Publication: https://academic.oup.com/bioinformatics/article/37/18/3067/6168117
  Published: true
  Task:
  - Causal Structure
  Year: 2021
- Code Availability: https://github.com/rmwu/sea
  Description: SEA predicts large causal graphs by leveraging small graphs generated
    from subsets of variables using standard causal discovery algorithms like FCI
    or GIES. To tackle the challenges of causal discovery with large variable sets,
    SEA employs an amortized learning approach and utilizes a complex architecture,
    including transformer modules and diverse embeddings, to aggregate the subgraphs.
    SEA is pre-trained on synthetic data with known causal structures and encodes
    interventions by replicating the effects on the encoded graph.
  Inspired by:
  - GIES
  - AVICI
  Method: SEA
  Model:
  - Amortized pre-training
  - Transfomer
  - 'Graph Attention '
  Publication: https://arxiv.org/abs/2402.01929
  Published: true
  Task:
  - Causal Structure
  Year: 2024
- Code Availability: https://github.com/ML4BM-Lab/SENA
  Description: "SENA replaces discrepancy\u2011VAE's encoder by using a gene-to-pathway\
    \ mask that applies a soft weighting, via the pathway activity scores \u03B1,\
    \ to the gene expression inputs. In this design, each weight in the encoder is\
    \ elementwise multiplied by a mask M that assigns full weight to genes known to\
    \ belong to a pathway and a tunable, lower weight (\u03BB) to genes outside the\
    \ pathway. This allows the model to primarily capture the signal of annotated\
    \ genes while still letting unannotated genes contribute, thereby forming interpretable\
    \ latent factors as linear combinations of pathway activities. "
  Inspired by:
  - Discrepancy-VAE
  Method: SENA
  Model:
  - Discrepancy-VAE architecture
  - VAE
  - PK Representations
  Publication: https://openreview.net/forum?id=NjlafBAahz
  Published: true
  Task:
  - Causal Structure
  - Multi-component Disentanglement
  - Seen Perturbation Prediction
  - Combinatorial Effect Prediction
  Year: 2024
- Code Availability: https://github.com/KrishnaswamyLab/RiTINI
  Description: RiTINI employs graph ordinary differential equations (graph-ODEs) to
    infer time-varying interaction graphs from multivariate time series data. RiTINI
    integrates dual attention mechanisms to enhance dynamic modeling and defines interaction
    graph inference as identifying a directed graph. Further, RiTINI utilizes prior
    knowledge to initialize the causal graph and by penalizing deviations the prior.Additionally,
    RiTINI simulates perturbations in silico to further refine the graph structure.
  Inspired by:
  - PC Algorithm
  Method: RiTINI
  Model:
  - Graph interventions
  - Graph-ODE
  Publication: https://proceedings.mlr.press/v231/bhaskar24a.html
  Published: true
  Task:
  - GRN Inference
  - Causal Structure
  - Context Transfer
  Year: 2024
- Code Availability: https://bitbucket.org/weililab/scmageck/src/master/
  Description: 'scMAGeCK is a framework with two modules: 1) scMAGeCK-RRA ranks cells
    by marker expression and uses rank aggregation, with a dropout filtering step,
    to detect enrichment of specific perturbations; 2) scMAGeCK-LR applies ridge regression
    on the expression matrix to compute the relevance of perturbations, including
    in cells with multiple perturbations. Both modules rely on permutation tests and
    Benjamini-Hochberg correction.'
  Inspired by:
  - '-'
  Method: scMAGeCK
  Model:
  - Robust Rank Aggregate
  - Ridge Regression
  Publication: https://link.springer.com/article/10.1186/s13059-020-1928-4#Sec10
  Published: true
  Task:
  - Differential Analysis
  Year: 2020
- Code Availability: https://github.com/Katsevich-Lab/sceptre
  Description: "For each gene-gRNA pair, SCEPTRE fits a negative binomial regression\
    \ where the response is the gene\u2019s expression across cells and the predictors\
    \ are binary indicator denoting gRNA presence, plus technical covariates. Concurrently,\
    \ a logistic regression using the same technical factors estimates \u03C0 - the\
    \ probability of detecting the gRNA in a cell. In a conditional resampling step,\
    \ gRNA assignments are independently redrawn per cell based on \u03C0, generating\
    \ an empirical null distribution of z-scores; a skew\u2011t distribution\
    \ is then fitted to this null to yield calibrated p\u2011values."
  Inspired by:
  - '-'
  Method: SPECTRE
  Model:
  - Conditional Resampling
  - Generalised Linear Model
  - NB Likelihood
  Publication: https://link.springer.com/article/10.1186/s13059-021-02545-2
  Published: true
  Task:
  - Differential Analysis
  - Perturbation Responsiveness
  Year: 2021
- Code Availability: https://longmanz.github.io/Mixscale/
  Description: "Mixscale extends Mixscape by converting the binary perturbed/non-\
    perturbed assignment into a continuous perturbation score. As it's predecessor,\
    \ it first identifies DE genes between gRNA-targeted and non-targeting control\
    \ cells, then computes perturbation vector and projects each cell\u2019s expression\
    \ profile onto this vector to yield a quantitative score (computed independently\
    \ per cell line). A weighted multivariate regression is then applied where each\
    \ cell\u2019s contribution is scaled according to its perturbation score, so that\
    \ cells with weaker perturbation (and thus lower scores) have reduced influence\
    \ on the model. This regression also incorporates covariates such as cell line\
    \ identity and sequencing depth, and uses a leave-one-feature-out procedure."
  Inspired by:
  - '-'
  Method: Mixscale
  Model:
  - Gaussian Mixture Model
  - Weighted multivariate regression
  Publication: https://www.nature.com/articles/s41556-025-01622-z
  Published: true
  Task:
  - Differential Analysis
  - Perturbation Responsiveness
  Year: 2025
- Code Availability: https://github.com/KrishnaswamyLab/MELD
  Description: "MELD models cells as samples drawn from a probability density defined\
    \ in a low-dimensional space (manifold). Each cell is assigned to a one-hot indicator\
    \ according to its sample origin (e.g. treatment or control), normalized by the\
    \ total cell count in that sample. A cell (transcriptomic) similarity graph is\
    \ then built using a decaying kernel, and the normalized indicator vectors are\
    \ smoothed across the graph, such that each cell\u2019s value is updated by averaging\
    \ with its neighbors to yield a density estimate for each sample (condition) for\
    \ that cell. Normalizing these estimates produces a perturbation-associated relative\
    \ likelihood for each cell. Vertex Frequency Clustering (VFC) then uses these\
    \ likelihoods, cell indicator vectors, and similarity graphs to cluster cells\
    \ with similar transcriptomics and perturbation profiles."
  Inspired by:
  - '-'
  Method: MELD(-VCF)
  Model:
  - Manifold Learning
  - Vertex-frequency analysis
  - Graph Diffusion
  Publication: https://www.nature.com/articles/s41587-020-00803-5#Sec13
  Published: true
  Task:
  - Perturbation Responsiveness
  Year: 2021
- Code Availability: https://github.com/csglab/GEDI
  Description: GEDI learns a shared latent space and, for each sample, estimates a
    specific reconstruction function that maps latent states to observed gene expression
    profiles. This design captures inter-sample variability and enables differential
    expression analysis along continuous cell-state gradients without relying on predefined
    clusters. Optionally, it can incorporate prior knowledge.
  Inspired by:
  - PLIER (PK representation)
  Method: GEDI
  Model:
  - Probabilistic
  - Sample-specific Decoders
  - PK Representations (optional)
  - RNA Velocity (optional)
  Publication: https://www.nature.com/articles/s41467-024-50963-0?fromPaywallRec=false
  Published: true
  Task:
  - Linear Gene Programmes
  Year: 2024
- Code Availability: https://github.com/yelabucsf/scrna-parameter-estimation
  Description: "Memento is a differential expression framework that uses method-of-moments\
    \ estimators under a multivariate hypergeometric model, where a gene\u2019s mean\
    \ is derived from Good-Turing corrected counts scaled by total cell counts. Differential\
    \ variability is quantified as the variance remaining after accounting for mean-dependent\
    \ effects (residual variance), while the covariance (pairwise association) between\
    \ genes is estimated from the off-diagonal elements of the resulting variance-covariance\
    \ matrix. Efficient permutation is achieved through a bootstrapping strategy that\
    \ leverages the sparsity of unique transcript counts."
  Inspired by:
  - '-'
  Method: Memento
  Model:
  - Hypergeometric test
  - Probabilistic
  Publication: https://www.cell.com/cell/fulltext/S0092-8674(24)01144-9
  Published: true
  Task:
  - Differential Analysis
  - Linear Gene Programmes
  Year: 2024
- Code Availability: https://github.com/kharchenkolab/scITD
  Description: "scITD constructs a three-dimensional tensor (donors \xD7 genes \xD7\
    \ cell types) by generating donor-by-gene pseudobulk matrices for each cell type.\
    \ Tucker decomposition then decomposes this tensor into separate factor matrices\
    \ for donors, genes, and cell types, along with a core tensor that captures their\
    \ interactions as latent multicellular expression patterns. The gene factors and\
    \ core tensor are rearranged into a loading tensor analogous to PCA loadings,\
    \ while the donor factor matrix represents sample scores. Finally, to improve\
    \ interpretability, a two-step rotation is carried out - first applying ICA to\
    \ the gene factors and then varimax to the donor factors."
  Inspired by:
  - Tucker Decomposition
  Method: scITD
  Model:
  - Tensor Decomposition (Tucker)
  Publication: https://www.nature.com/articles/s41587-024-02411-z
  Published: true
  Task:
  - Linear Gene Programmes
  Year: 2024
- Code Availability: https://github.com/ZJUFanLab/scRank
  Description: scRank infers cell type-specific Gene Programmes from untreated scRNA-seq
    data by constructing co-expression networks via principal component regression
    with random subsampling and integrating them using tensor decomposition. It simulates
    drug perturbation by modifying the drug targets' outgoing edges to generate an
    in-sillico perturbed network, and then aligns the untreated and perturbed networks
    via Laplacian eigen-decomposition. In this low-dimensional space, the distances
    between corresponding gene nodes quantify gene-level changes due to the perturbation.
    These distances, weighted by network connectivity (e.g., outgoing edge strength
    normalized by node degree) and extended through two-hop diffusion, are aggregated
    to yield a composite perturbation score that ranks cell types by their predicted
    drug responsiveness.
  Inspired by:
  - '-'
  Method: scRANK
  Model:
  - PC Regression
  - Tensor Decomposition (PARAFAC)
  - Network Diffusion
  Publication: https://www.cell.com/cell-reports-medicine/fulltext/S2666-3791(24)00260-X
  Published: true
  Task:
  - Linear Gene Programmes
  - Perturbation Responsiveness
  - GRN Inference
  Year: 2024
- Code Availability: https://github.com/C0nc/TAICHI
  Description: Taichi identifies perturbation-relevant cell niches in spatial omics
    data without predefined spatial clustering. It first constructs spatially-informed
    embeddings using MENDER, which are then used in a logistic regression model to
    predict slice-level condition labels. Using the trained model each cell (niche)
    is assigned a probability of belonging to the condition group. These probabilities
    are clustered using k-means (k=2) to separate condition-relevant and control-like
    niches. Graph heat diffusion is applied to refine these labels by propagating
    information across spatially adjacent cells. Finally, a second k-means clustering
    step is performed on the diffused results to define the final niche segmentation.
  Inspired by:
  - MELD
  Method: Taichi
  Model:
  - Graph Diffusion
  - K-means
  - Logistic Regression
  - Spatially-informed
  Publication: https://www.biorxiv.org/content/10.1101/2024.05.30.596656v1.abstract
  Published: false
  Task:
  - Perturbation Responsiveness
  - Perturbation Responsiveness
  - Differential Analysis
  Year: 2024
- Code Availability: https://github.com/C0nc/River
  Description: River identifies condition-relevant genes from spatial omics data across
    multiple slices or conditions. It learns gene expression and spatial coordinate
    embeddings using separate MLP-based encoders, which are then concatenated and
    used to predict condition labels. Spatial alignment is thus required as a preprocessing
    step. In a second step, River uses Integrated Gradients, DeepLift, and GradientShap
    to attribute model predictions to input genes at the cell level. These attribution
    scores are aggregated using rank aggregation to prioritize condition-relevant
    genes.
  Inspired by:
  - '-'
  Method: River
  Model:
  - Non-linear Classifier
  - Feature Attribution
  - Spatially-informed
  Publication: https://www.biorxiv.org/content/10.1101/2024.08.04.606512v1.abstract
  Published: false
  Task:
  - Differential Analysis
  Year: 2024
- Code Availability: https://github.com/bm2-lab/MUSIC
  Description: MUSIC evaluates sgRNA knockout efficiency and summarises perturbation
    effects using topic modeling. Following preprocessing steps, MUSIC removes low-efficiency
    (non-targeted) cells based on the cosine similarity of their differential expression
    genes, excluding perturbed cells with profiles more similar to controls. Next,
    highly dispersed DE genes are selected and their normalized expression values
    are used as to fit a topic model, where cells are treated as documents and gene
    counts as words. Topics are then ranked according to overall effect, their relevance
    to each perturbation, and perturbation similarities.
  Inspired by:
  - LDA
  - Correlated topic model
  Method: MUSIC
  Model:
  - Topic Model
  Publication: https://www.nature.com/articles/s41467-019-10216-x
  Published: true
  Task:
  - Perturbation Responsiveness
  - Linear Gene Programmes
  Year: 2019
- Code Availability: https://github.com/satijalab/seurat
  Description: "Mixscape aims to classify CRISPR-targeted cells into perturbed and\
    \ not perturbed (escaping). To eachive that, Mixscape computes a local perturbation\
    \ signature by subtracting each cell\u2019s mRNA expression from the average of\
    \ its k nearest NT (non-targeted) control neighbors. Differential expression testing\
    \ between targeted and NT cells then identifies a set of DEGs that capture the\
    \ perturbation response. These DEGs are used to define a perturbation vector-essentially,\
    \ the average difference in expression between targeted and NT cells, which projects\
    \ each cell\u2019s DEG expression onto a single perturbation score. The Gaussian\
    \ mixture model is applied to these perturbation scores, with one component fixed\
    \ to match the NT distribution, while the other represents the perturbation effect.\
    \ This model assigns probabilities that classify each targeted cell as either\
    \ perturbed or escaping. Additionally, the authors propose visualisation with\
    \ Linear Discriminant Analysis and UMAP, aiming to identify a low-dimensional\
    \ subspace that maximally discriminates the mixscape-derived classes."
  Inspired by:
  - '-'
  Method: Mixscape
  Model:
  - Gaussian Mixture Model
  - 'LDA

    '
  Publication: https://www.nature.com/articles/s41588-021-00778-2#Sec11
  Published: true
  Task:
  - Perturbation Responsiveness
  - Linear Gene Programmes
  Year: 2021
- Code Availability: https://github.com/davidliwei/PS
  Description: "Perturbation Score (PS) quantifies single-cell responses to (\"dosage\"\
    -informed) perturbations in three steps. First, a signature gene set is defined\
    \ for each perturbation - either via differential expression or pre-defined gene\
    \ sets. Second, scMAGeCK\u2019s regression framework estimates gene-level coefficients\
    \ (\u03B2) reflecting the average effect of each perturbation on its target genes.\
    \ Third, a regularised regression model is fitted per cell to estimate a scalar\
    \ PS [0-to-1], reflecting how well the cell\u2019s gene expression profile matches\
    \ the \u03B2-weighted perturbation signature. This is done through constrained\
    \ optimisation, where scores are inferred only for cells annotated as receiving\
    \ the perturbation."
  Inspired by:
  - scMageck (theirs)
  Method: Perturbation Score
  Model:
  - Robust Rank Aggregate
  - Ridge Regression
  - Regularised Linear Models
  Publication: https://www.nature.com/articles/s41556-025-01626-9
  Published: true
  Task:
  - Perturbation Responsiveness
  - Differential Analysis
  Year: 2025
- Code Availability: https://github.com/Katsevich-Lab/sceptre
  Description: "For each gene-gRNA pair, SCEPTRE fits a negative binomial regression\
    \ where the response is the gene\u2019s expression across cells and the predictors\
    \ are binary indicator denoting gRNA presence, plus technical covariates. Concurrently,\
    \ a logistic regression using the same technical factors estimates \u03C0 - the\
    \ probability of detecting the gRNA in a cell. In a conditional resampling step,\
    \ gRNA assignments are independently redrawn per cell based on \u03C0, generating\
    \ an empirical null distribution of z-scores; a skew\u2011t distribution\
    \ is then fitted to this null to yield calibrated p\u2011values."
  Inspired by:
  - '-'
  Method: SCEPTRE
  Model:
  - Conditional Resampling
  - Generalised Linear Model
  - NB Likelihood
  Publication: https://link.springer.com/article/10.1186/s13059-021-02545-2#Sec11
  Published: true
  Task:
  - Perturbation Responsiveness
  - Differential Analysis
  Year: 2021
- Code Availability: https://github.com/saezlab/MOFAcellulaR
  Description: 'Multicellular factor analysis repurposes MOFA by treating pseudobulked
    cell types as views. Each patient is represented by multiple views - one per cell
    type - summarizing gene expression. MOFA+ ised then used to identify latent factors
    that capture coordinated variability across these views, with loadings indicating
    cell-type-specific gene contributions. '
  Inspired by:
  - MOFA+
  Method: MOFAcell
  Model:
  - Group Factor Analysis (MOFA+)
  Publication: https://elifesciences.org/articles/93161
  Published: true
  Task:
  - Linear Gene Programmes
  Year: 2023
- Code Availability: https://github.com/neurorestore/Vespucci
  Description: 'Vespucci builds on Augur, and similarly it trains a random forest
    classifier to predict perturbation labels based on gene expression but extends
    this to spatial barcodes, using cross-validation within small, neighbouring regions
    to compute the area under the ROC curve (AUC) as a measure of transcriptional
    separability per observation. To overcome the computational inefficiency of classification
    across all observations, Vespucci employs a meta-learning approach: it first performs
    exhaustive classification on a subset of barcodes, then trains a random forrest
    regression model on derived distance metrics (e.g., Pearson correlation, Spearman
    correlation) between all pairs of observations to impute AUCs across the full
    dataset. This is done by iteratively expanding the number of observations in the
    training set until convergence (according to prediction similarity to the previous
    iteration). Finally, perturbation-responsive genes are identified by splitting
    the data (using an independent set of observations) to avoid bias, then using
    negative binomial mixed models to link gene expression to AUC scores.'
  Inspired by:
  - Augur (theirs)
  Method: Vespucci
  Model:
  - Random Forrest
  - Spatially-Informed
  Publication: https://www.biorxiv.org/content/10.1101/2024.06.13.598641v2.full
  Published: false
  Task:
  - Perturbation Responsiveness
  - Differential Analysis
  Year: 2024
- Code Availability: '-'
  Description: 'GEASS is a causal feature selection framework in high-dimensional
    spatal & temporal omics data that identifies nonlinear Granger causal interactions
    by maximizing a sparsity-regularized modified transfer entropy. It enforces sparsity
    using combinatorial stochastic gate layers that allow it to select a minimal subset
    of features with causal interactions - i.e. two sets of of non-overlapping genes
    as drivers (source) and receivers (sink). '
  Inspired by:
  - Transfer Entropy
  Method: GEASS
  Model:
  - Non-linear Granger Causality
  - Stochastic Gate Layers (Feature Selectors)
  - Time-resolved / Spatially-informed
  Publication: https://openreview.net/forum?id=aKcS3xojnwY
  Published: true
  Task:
  - Nonlinear Gene Programmes
  Year: 2023
- Code Availability: https://github.com/MarioniLab/miloDE
  Description: 'GEASS is a causal feature selection framework in high-dimensional
    spatal & temporal omics data that identifies nonlinear Granger causal interactions
    by maximizing a sparsity-regularized modified transfer entropy. It enforces sparsity
    using combinatorial stochastic gate layers that allow it to select a minimal subset
    of features with causal interactions - i.e. two sets of of non-overlapping genes
    as drivers (source) and receivers (sink). '
  Inspired by:
  - Milo (theirs)
  - edgeR
  - Cydar
  Method: MiloDE
  Model:
  - Generalised Linear Model
  - NB Likelihood
  Publication: https://link.springer.com/article/10.1186/s13059-024-03334-3
  Published: true
  Task:
  - Differential Analysis
  Year: 2024
- Code Availability: https://github.com/YosefLab/Hotspot/tree/master
  Description: Hotspot proposes a modified autocorrelation metrics that detect genes
    with coherent expression among neighboring cells (K-nearest neighbours graph in
    a latent space, spatial proximities, or lineage). By comparing these local autocorrelation
    scores to a permutation-free null model (e.g. using negative binomial or Bernoulli
    assumptions), it calculates the significance of autocorrelated genes. Additionally,
    for module detection, Hotspot computes pairwise correlations that capture how
    similarly two genes are expressed across nearby cells and then applies hierarchical
    clustering to group genes into biologically coherent modules.
  Inspired by:
  - Spatial Autocorrelation Metrics (e.g. Morans I)
  Method: Hotspot
  Model:
  - Autocorrelation
  - Pairwise Local Correlations
  Publication: https://www.sciencedirect.com/science/article/pii/S2405471221001149
  Published: true
  Task:
  - Nonlinear Gene Programmes
  Year: 2021
- Code Availability: https://github.com/livnatje/DIALOGUE
  Description: "DIALOGUE identifies shared multicellular patterns across cell types\
    \ and samples. It first constructs cell-type\u2013specific data matrices by averaging\
    \ features (e.g., gene expression or PCs) over samples or spatial niches. Then\
    \ it applies multi-factor sparse canonical correlation analysis (referred to as\
    \ penalized matrix decomposition (PMD)) to derive latent feature matrices that\
    \ maximize cross-cell-type correlations under LASSO constraints. Following this\
    \ initial PMD step, DIALOGUE employs correlation coefficients and permutation\
    \ tests to determine which cell types contribute to each multicellular progarmmes\
    \ (MCP). It then re-applies the PMD procedure in both a multi-way and a pairwise\
    \ fashion, incorporating programs unique to the pairwise analysis into the downstream\
    \ modeling. Finally, gene associated with MCPs are first identified using partial\
    \ Spearman correlation and then refined through hierarchical mixed-effects modeling\
    \ with covariate control."
  Inspired by:
  - CCA
  Method: DIALOGUE
  Model:
  - Sparse CCA
  - Partial Correlations
  - Mixed Linear Model
  Publication: https://www.nature.com/articles/s41587-022-01288-0
  Published: true
  Task:
  - Linear Gene Programmes
  Year: 2022
- Code Availability: https://github.com/neurorestore/Augur
  Description: "Augur rank cell types by quantifying how accurately perturbation labels\
    \ can be predicted from gene expression profiles using a random forest classifier\
    \ (or regressor depending on the perturbation label). For each cell type, it repeatedly\
    \ subsamples a fixed number of cells to mitigate biases from uneven cell numbers.\
    \ It also employs a two-step feature selection procedure, first, identifying highly\
    \ variable genes via local polynomial regression on the mean\u2013variance relationship,\
    \ and second, random downsampling. AUGUR then uses cross-validation to compute\
    \ the area under the ROC curve (AUC) as a model performance metric that is used\
    \ to quantify the perturbation effect on each cell type. It also provides (gene)\
    \ feature importances. For multi-class or continous perturbations, cell-type effects\
    \ (model performance) are measured using macro-averaged AUC or concordance correlation\
    \ coefficient, respectively."
  Inspired by:
  - '-'
  Method: AUGUR
  Model:
  - Random Forrest
  Publication: https://www.nature.com/articles/s41587-020-0605-1#Abs1
  Published: true
  Task:
  - Perturbation Responsiveness
  - Differential Analysis
  Year: 2020
- Code Availability: https://github.com/phillipnicol/scDist
  Description: scDist is a statistical framework that uses linear mixed-effects models
    to estimate gene-level condition effects while accounting for individual and technical
    variability. In the model, baseline expression levels are first captured, and
    then a parameter representing the condition-induced change is estimated. The overall
    shift between conditions is quantified by computing the Euclidean distance between
    the condition-specific mean expression profiles - essentially, by taking the norm
    of the condition effect vector. This high-dimensional metric is then efficiently
    approximated in a lower-dimensional space via principal component analysis.
  Inspired by:
  - AUGUR
  Method: scDIST
  Model:
  - Generalised Linear Model
  Publication: https://www.nature.com/articles/s41467-024-51649-3
  Published: true
  Task:
  - Perturbation Responsiveness
  - Differential Analysis
  Year: 2024
- Code Availability: https://github.com/czi-ai/sub-cell-embed
  Description: SubCell is a set of self-supervised vision transformer (ViT) models
    trained on low-plex single-cell immunofluorescence images from the Human Protein
    Atlas to learn biologically meaningful representations of protein localisation
    and tissue morphology. The models are optimised using a multi-task objective that
    combines masked autoencoding for spatial reconstruction, a cell-specific contrastive
    loss to enforce consistency across augmented views of the same cell, and a protein-specific
    contrastive loss to align embeddings of cells stained for the same protein across
    different cell lines and experiments. An attention pooling module is used to priotise
    informative subcellular regions. The resulting models, particularly ViT-ProtS-Pool
    and MAE-CellS-ProtS-Pool, are shown to generalise across datasets, imaging modalities,
    cell types, and perturbations without fine-tuning.
  Inspired by:
  - Contrastive Masked Autoencoders
  - Contrastive Learning(Chen et al.)
  Method: SubCell
  Model:
  - A collection of Vision Transformer Models
  - Contrastive loss
  Publication: https://www.biorxiv.org/content/10.1101/2024.12.06.627299v1.abstract
  Published: false
  Task:
  - Context Transfer
  - Nonlinear Gene Programmes
  Year: 2024
- Code Availability: https://github.com/bunnelab/virtues
  Description: VirTues is a multi-modal foundation model based on a vision transformer
    architecture, trained on multiplex spatial proteomics data from lung, breast,
    and melanoma tumors. It combines image representations with protein language model
    (PLM) embeddings of molecular markers and constructs hierarchical summary tokens
    at the cell, niche, and tissue levels. This PLM-based tokenisation enables the
    model to predict previously unseen markers. The architecture employs a sparse
    attention mechanism that factorises attention into spatial and marker components
    to manage the computational complexity of high-dimensional input. Training is
    performed using a masked autoencoding objective - i.e. it reconstructs missing
    subsets of spatial and protein data.
  Inspired by:
  - Masked Autoencoders
  Method: VirTues
  Model:
  - ViT
  - Maked Autoencoder
  - Multi-modal
  Publication: https://arxiv.org/abs/2501.06039
  Published: false
  Task:
  - Context Transfer
  Year: 2025
- Code Availability: https://github.com/GuangyuWangLab2021/Loki
  Description: "OmiCLIP is a dual-encoder foundation model that encodes H&E histology\
    \ patches with a Vision Transformer and \u201Cgene-sentence\u201D representations\
    \ of (10X Visium) spatial transcriptomics (ST) using a Transformer initialised\
    \ on LAION-5B. It projects both modalities into a shared latent space using symmetric\
    \ contrastive learning, which pulls matched histology-transcriptome pairs. The\
    \ model is pretrained on paired H&E images and ST data across diverse organs and\
    \ disease states. These aligned embeddings underpin downstream tasks such as spatial\
    \ registration of histology to transcriptomic spots, zero-shot tissue annotation,\
    \ cell-type deconvolution, retrieval of matching transcriptomic profiles for novel\
    \ histology inputs, and prediction of spatial gene-expression patterns directly\
    \ from histology."
  Inspired by:
  - CLIP
  - CoCa(Yu et al)
  - Cell2Setence
  - GenePT
  Method: OmiCLIP
  Model:
  - ViT
  - Contrastive Learning
  - Multi-modal
  Publication: https://www.nature.com/articles/s41592-025-02707-1
  Published: true
  Task:
  - Context Transfer
  Year: 2025
- Code Availability: https://github.com/theislab/prophet
  Description: "Prophet represents each experiment as a set of three axes - cellular\
    \ state (cell lines), treatments (perturbations), and phenotypic readouts - and\
    \ projects diverse prior knowledge types (e.g., CCLE bulk RNA-seq for cell lines;\
    \ chemical fingerprints or transcriptomic/genomic vectors for perturbations; learnable\
    \ embeddings for readouts) into a shared token space. It is pre-trained on a set\
    \ of diverse perturbation experiments covering readouts such as cell viability,\
    \ compound IC50, Cell Painting morphology features, mRNA transcript abundance,\
    \ and cell type proportions. A transformer-based encoder integrates these tokenised\
    \ inputs, feeding a regression head that\u2019s trained end-to-end to minimise\
    \ mean squared error across all outcome types. The model is fine-tuned for assay-specific\
    \ data applications."
  Inspired by:
  - '-'
  Method: Prophet
  Model:
  - Transformer
  - Multi-modal
  - Knowledge Informed
  Publication: https://www.biorxiv.org/content/10.1101/2024.08.12.607533v2
  Published: false
  Task:
  - Unseen Perturbation Prediction
  - Context Transfer
  Year: 2024
- Code Availability: https://github.com/Genentech/iterative-perturb-seq
  Description: "Iterpert is an active learning framework for Perturb-seq experiments\
    \ that uses GEARS to predict gene expression gene expression perturbation effects.\
    \ The method iteratively retrains GEARS on new data and selects the next batch\
    \ of perturbations using an enhanced kernel, which is constructed by fusing the\
    \ GEARS-derived kernel with kernels from six prior information sources (additional\
    \ Perturb-seq data, optical pooled screens, scRNA-seq atlases, protein structures,\
    \ protein\u2013protein interaction networks, and literature-derived features).\
    \ Each prior source is mapped into a kernel matrix, normalized, and combined with\
    \ the model kernel via a mean fusion operator. The fused kernel is then used with\
    \ a greedy distance maximisation rule to select perturbation batches under budget\
    \ constraints (a limited set of experiments per round). "
  Inspired by:
  - GEARS
  Method: IterPert
  Model:
  - GEARS
  - Active Learning
  Publication: https://www.biorxiv.org/content/10.1101/2023.12.12.571389v1.full.pdf
  Published: false
  Task:
  - Unseen Perturbation Prediction
  - Combinatorial Effect Prediction
  Year: 2024
- Code Availability: https://github.com/azizilab/decipher
  Description: Decipher is a hierarchical deep generative model to integrate and visualize
    single-cell RNA-seq data from both normal and perturbed conditions, identifying
    shared and disrupted cell-state trajectories. Its architecture includes dual latent
    spaces -a low-dimensional state for detailed cell-state modeling and a two-dimensional
    space for visualisation-connected to gene expression through linear or single-layer
    neural network transformations. The model aligns trajectories by maintaining shared
    transcriptional programs for common biological processes across conditions.
  Inspired by:
  - '-'
  Method: Decipher
  Model:
  - VAE
  - Linear Decoder
  Publication: https://www.biorxiv.org/content/10.1101/2023.11.11.566719v2.full
  Published: false
  Task:
  - Unsupervised Disentanglement
  - Linear Gene Programmes
  Year: 2024
- Code Availability: https://github.com/hsmaan/SpatialDIVA
  Description: SpatialDIVA learns distinct latent spaces capturing intrinsic (transcriptomic),
    morphological (histology), spatial neighborhood, technical (batch), and residual
    variations. To promote disentanglement, the model employs auxiliary classification
    heads - using cell type labels to supervise the transcriptomic latent space, pathology
    annotations to guide the histology latent space, and batch labels to capture technical
    variation. Additionally, an auxiliary regression head with mean squared error
    (MSE) loss is trained to ensure that the spatial latent space accurately reconstructs
    a PCA-based representation derived from concatenated histology and transcriptomic
    profiles from k-nearest spatial neighbours, thereby capturing both imaging and
    expression data from adjacent spots.
  Inspired by:
  - DIVA
  Method: SpatialDIVA
  Model:
  - VAE
  - Spatially-informed
  - NB Likelihood
  - Multi-modal
  Publication: https://www.biorxiv.org/content/10.1101/2025.02.19.638201v1.full.pdf
  Published: false
  Task:
  - Multi-component Disentanglement
  Year: 2025
- Code Availability: https://github.com/Bertinus/FLeCS
  Description: "FLeCS models single-cell gene expression dynamics using coupled ordinary\
    \ differential equations (ODEs) parameterized by a gene regulatory network. Cells\
    \ are grouped into temporal bins\u2014either via pseudotime inference or experimental\
    \ timestamps\u2014and aligned across time with optimal transport to form (pseudo)time\
    \ series. To model interventions FLeCS replicates interventions in the learned\
    \ graph."
  Inspired by:
  - '-'
  Method: FLeCS
  Model:
  - ODE
  - Optimal Transp
  Publication: https://arxiv.org/pdf/2503.20027
  Published: false
  Task:
  - Context Transfer
  - GRN Inference
  - Causal Structure
  Year: 2025
- Code Availability: https://github.com/masastat/RENGE
  Description: RENGE attempts to infer gene regulatory networks from time-series single-cell
    CRISPR knockout data. It models changes in gene expression following a knockout
    by propagating the effects through direct and higher-order (indirect) regulatory
    paths, where the gene network is represented as a matrix of regulatory strengths
    between gene pairs.
  Inspired by:
  - '-'
  Method: RENGE
  Model:
  - Regression model
  Publication: https://www.nature.com/articles/s42003-023-05594-4
  Published: true
  Task:
  - Context Transfer
  - GRN Inference
  - Causal Structure
  Year: 2023
- Code Availability: https://github.com/daifengwanglab/ARTEMIS
  Description: "ARTEMIS combines variational autoencoder with an unbalanced Diffusion\
    \ Schr\xF6dinger Bridge to reconstruct continuous trajectories from time-series\
    \ data. The methods trains a VAE first, then jointly trains the VAE and uDSB by\
    \ solving the forward-backward SDEs in the latent space using neural networks."
  Inspired by:
  - '-'
  Method: ARTEMIS
  Model:
  - VAE
  - "Schr\xF6dinger Bridge"
  Publication: https://www.biorxiv.org/content/biorxiv/early/2025/01/26/2025.01.23.634618.full.pdf
  Published: false
  Task:
  - Trace Cell Populations
  Year: 2025
- Code Availability: https://github.com/jiang-q19/scPRAM
  Description: 'scPRAM is a computational framework for predicting single-cell gene
    expression changes in response to perturbations. The method integrates three main
    components: a variational autoencoder (VAE), optimal transport, and an attention
    mechanism. The VAE encodes the gene expression data into a latent space. Optimal
    transport is applied in this latent space to match unpaired cells before and after
    perturbation by finding an optimal coupling between their distributions. For each
    test cell, the attention mechanism computes a perturbation vector by comparing
    its latent representation (query) against those of matched training cells (keys
    and values). The predicted post-perturbation response is generated by adding the
    perturbation vector to the query and decoding it back to gene expression space
    using the VAE decoder.'
  Inspired by:
  - '-'
  Method: scPRAM
  Model:
  - VAE
  - OT
  - Attention
  Publication: https://academic.oup.com/bioinformatics/article/40/5/btae265/7646141
  Published: true
  Task:
  - Context Transfer
  - Trace Cell Populations
  Year: 2024
- Code Availability: https://saezlab.github.io/mistyR/
  Description: MISTy extracts intra- and intercellular relationships from spatial
    omics data by learning multivariate interactions through a multi-view approach,
    where each view represents a collection of variables (e.g., a modality or an aggragation
    of a spatial niche). It jointly models spatial and functional aspects of the data,
    supporting any number of views with arbitrary numbers of variables. Target variables
    (intrinsic view) are predicted using random forests (by default), either via leave-feature-one-out
    within the intrinsic view or using the remaining (extrinsic) views.
  Inspired by:
  - SVCA
  Method: MISTy
  Model:
  - Spatially-informed
  - Random Forrest (or other regression models)
  Publication: https://link.springer.com/article/10.1186/s13059-022-02663-5
  Published: true
  Task:
  - Feature Relationships
  Year: 2022
- Code Availability: https://github.com/sschrod/SpaCeNet
  Description: SpaCeNet aims to untangle the complex relationships between molecular
    interactions within and between cells by analyzing spatially resolved single-cell
    data. To achieve this, SpaCeNet leverages an adaptation of probabilistic graphical
    models  to enable spatially resolved conditional independence testing. This approach
    allows for the identification of direct and indirect dependencies, as well as
    the removal of spurious gene association patterns. Additionally, SpaCeNet incorporates
    explicit cell-cell distance information to differentiate between short- and long-range
    interactions, thereby distinguishing between baseline cellular variability and
    interactions influenced by a cell's microenvironment.
  Inspired by:
  - '-'
  Method: SpaCeNet
  Model:
  - Generalised Gaussian Graphical Model
  - Spatially-informed
  Publication: https://genome.cshlp.org/content/34/9/1371
  Published: true
  Task:
  - Feature Relationships
  Year: 2024
- Code Availability: https://www.github.com/jtanevski/kasumi
  Description: Kasumi extends MISTy by focusing on identifying localised relationship
    patterns that are persistent across tissue samples. Instead of modeling global
    relationships, it uses a sliding-window approach to learn representations of local
    tissue patches (neighborhoods), characterized by multivariate, potentially non-linear
    relationships across views. These window-specific relationship signatures are
    clustered (using graph-based community detection) into spatial patterns, which
    are consistently observed across multiple samples. This enables Kasumi to represent
    each sample as a distribution over interpretable, shared local patterns, facilitating
    tasks like patient stratification while maintaining model explainability.
  Inspired by:
  - MISTy
  Method: Kasumi
  Model:
  - Spatially-informed
  - Random Forrest (or other regression models)
  - Convolution Operations
  Publication: https://www.nature.com/articles/s41467-025-59448-0
  Published: true
  Task:
  - Feature Relationships
  Year: 2025
- Code Availability: https://github.com/gifford-lab/prescient
  Description: PRESCIENT models cellular differentiation as a stochastic diffusion
    process, where the drift term is parameterised as the negative gradient of a neural
    network-learned potential function. The model is trained using time-series single-cell
    RNA-seq data, and fits the potential function by minimizing the regularised Wasserstein
    distance between simulated and observed cell populations at each time point, explicitly
    incorporating cell proliferation by weighting cells according to their expected
    number of descendants. PRESCIENT can simulate differentiation trajectories for
    both observed and (in silico) perturbed cell states, enabling the prediction of
    cell fate outcomes under various genetic interventions.
  Inspired by:
  - '-'
  Method: Prescient
  Model:
  - Diffusion
  Publication: https://www.nature.com/articles/s41467-021-23518-w
  Published: true
  Task:
  - Trace Cell Populations
  - Seen Perturbation Prediction
  - Context Transfer
  Year: 2021
